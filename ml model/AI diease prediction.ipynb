{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c756806c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Allergy</th>\n",
       "      <th>Symptoms</th>\n",
       "      <th>Duration of Symptoms</th>\n",
       "      <th>Current Disease Info</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>BP Range</th>\n",
       "      <th>Temp Range</th>\n",
       "      <th>Disease</th>\n",
       "      <th>Prescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thrissur</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>26</td>\n",
       "      <td>None</td>\n",
       "      <td>Fever, Cough, Fatigue</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>No</td>\n",
       "      <td>120/80</td>\n",
       "      <td>37.5°C</td>\n",
       "      <td>Common Cold</td>\n",
       "      <td>Rest, Plenty of Fluids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kozhikode</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>Pollen</td>\n",
       "      <td>Sneezing, Itchy Eyes</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>No</td>\n",
       "      <td>130/90</td>\n",
       "      <td>36.9°C</td>\n",
       "      <td>Allergic Rhinitis</td>\n",
       "      <td>Antihistamines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alappuzha</td>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>None</td>\n",
       "      <td>Headache, Sore Throat</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>Accountant</td>\n",
       "      <td>No</td>\n",
       "      <td>110/70</td>\n",
       "      <td>37.2°C</td>\n",
       "      <td>Tonsillitis</td>\n",
       "      <td>Analgesics, Antibiotics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ernakulam</td>\n",
       "      <td>Female</td>\n",
       "      <td>50</td>\n",
       "      <td>29</td>\n",
       "      <td>None</td>\n",
       "      <td>Fatigue, Joint Pain, Muscle Aches</td>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "      <td>Nurse</td>\n",
       "      <td>No</td>\n",
       "      <td>125/80</td>\n",
       "      <td>37.8°C</td>\n",
       "      <td>Influenza</td>\n",
       "      <td>Bed Rest, Antiviral Medication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Malappuram</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>31</td>\n",
       "      <td>None</td>\n",
       "      <td>Chest Pain, Shortness of Breath</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>Doctor</td>\n",
       "      <td>No</td>\n",
       "      <td>135/85</td>\n",
       "      <td>37.4°C</td>\n",
       "      <td>Coronary Artery Disease</td>\n",
       "      <td>Aspirin, Statins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Kottayam</td>\n",
       "      <td>Male</td>\n",
       "      <td>38</td>\n",
       "      <td>26</td>\n",
       "      <td>None</td>\n",
       "      <td>Fever, Sore Throat, Cough</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>Teacher</td>\n",
       "      <td>No</td>\n",
       "      <td>130/90</td>\n",
       "      <td>37.6°C</td>\n",
       "      <td>Pharyngitis</td>\n",
       "      <td>Analgesics, Antibiotics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Kannur</td>\n",
       "      <td>Female</td>\n",
       "      <td>58</td>\n",
       "      <td>29</td>\n",
       "      <td>Dust</td>\n",
       "      <td>Sneezing, Itchy Eyes</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>Nurse</td>\n",
       "      <td>No</td>\n",
       "      <td>120/80</td>\n",
       "      <td>37.4°C</td>\n",
       "      <td>Allergic Rhinitis</td>\n",
       "      <td>Antihistamines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Pathanamthitta</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>27</td>\n",
       "      <td>None</td>\n",
       "      <td>Fatigue, Shortness of Breath</td>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "      <td>Pharmacist</td>\n",
       "      <td>No</td>\n",
       "      <td>115/75</td>\n",
       "      <td>37.0°C</td>\n",
       "      <td>Asthma</td>\n",
       "      <td>Bronchodilators, Inhalers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Kasaragod</td>\n",
       "      <td>Female</td>\n",
       "      <td>50</td>\n",
       "      <td>28</td>\n",
       "      <td>None</td>\n",
       "      <td>Cough, Sore Throat, Runny Nose</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>Accountant</td>\n",
       "      <td>No</td>\n",
       "      <td>125/80</td>\n",
       "      <td>36.8°C</td>\n",
       "      <td>Upper Respiratory Infection</td>\n",
       "      <td>Rest, Fluids, Cough Syrup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Idukki</td>\n",
       "      <td>Male</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>None</td>\n",
       "      <td>Fatigue, Headache, Runny Nose</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>IT Professional</td>\n",
       "      <td>No</td>\n",
       "      <td>130/90</td>\n",
       "      <td>37.5°C</td>\n",
       "      <td>Common Cold</td>\n",
       "      <td>Rest, Plenty of Fluids</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Location  Gender  Age  BMI Allergy  \\\n",
       "0         Thrissur    Male   35   26    None   \n",
       "1        Kozhikode  Female   42   30  Pollen   \n",
       "2        Alappuzha    Male   28   23    None   \n",
       "3        Ernakulam  Female   50   29    None   \n",
       "4       Malappuram    Male   45   31    None   \n",
       "..             ...     ...  ...  ...     ...   \n",
       "82        Kottayam    Male   38   26    None   \n",
       "83          Kannur  Female   58   29    Dust   \n",
       "84  Pathanamthitta    Male   35   27    None   \n",
       "85       Kasaragod  Female   50   28    None   \n",
       "86          Idukki    Male   32   24    None   \n",
       "\n",
       "                             Symptoms  Duration of Symptoms  \\\n",
       "0               Fever, Cough, Fatigue                     5   \n",
       "1                Sneezing, Itchy Eyes                     3   \n",
       "2               Headache, Sore Throat                     2   \n",
       "3   Fatigue, Joint Pain, Muscle Aches                     7   \n",
       "4     Chest Pain, Shortness of Breath                     1   \n",
       "..                                ...                   ...   \n",
       "82          Fever, Sore Throat, Cough                     5   \n",
       "83               Sneezing, Itchy Eyes                     3   \n",
       "84       Fatigue, Shortness of Breath                     7   \n",
       "85     Cough, Sore Throat, Runny Nose                     5   \n",
       "86      Fatigue, Headache, Runny Nose                     6   \n",
       "\n",
       "   Current Disease Info       Occupation Smoker BP Range Temp Range  \\\n",
       "0                  None         Engineer     No   120/80     37.5°C   \n",
       "1                  None          Teacher     No   130/90     36.9°C   \n",
       "2                  None       Accountant     No   110/70     37.2°C   \n",
       "3                  None            Nurse     No   125/80     37.8°C   \n",
       "4                  None           Doctor     No   135/85     37.4°C   \n",
       "..                  ...              ...    ...      ...        ...   \n",
       "82                 None          Teacher     No   130/90     37.6°C   \n",
       "83                 None            Nurse     No   120/80     37.4°C   \n",
       "84                 None       Pharmacist     No   115/75     37.0°C   \n",
       "85                 None       Accountant     No   125/80     36.8°C   \n",
       "86                 None  IT Professional     No   130/90     37.5°C   \n",
       "\n",
       "                        Disease                    Prescription  \n",
       "0                   Common Cold          Rest, Plenty of Fluids  \n",
       "1             Allergic Rhinitis                  Antihistamines  \n",
       "2                   Tonsillitis         Analgesics, Antibiotics  \n",
       "3                     Influenza  Bed Rest, Antiviral Medication  \n",
       "4       Coronary Artery Disease                Aspirin, Statins  \n",
       "..                          ...                             ...  \n",
       "82                  Pharyngitis         Analgesics, Antibiotics  \n",
       "83            Allergic Rhinitis                  Antihistamines  \n",
       "84                       Asthma       Bronchodilators, Inhalers  \n",
       "85  Upper Respiratory Infection       Rest, Fluids, Cough Syrup  \n",
       "86                  Common Cold          Rest, Plenty of Fluids  \n",
       "\n",
       "[87 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "data = pd.read_csv(\"data.csv\")  # Replace \"your_data.csv\" with the actual filename\n",
    "target_columns = [\"Disease\", \"Prescription\"]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bca2675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Location',\n",
       " 'Gender',\n",
       " 'Age',\n",
       " 'BMI',\n",
       " 'Allergy',\n",
       " 'Symptoms',\n",
       " 'Duration of Symptoms',\n",
       " 'Current Disease Info',\n",
       " 'Occupation',\n",
       " 'Smoker',\n",
       " 'BP Range',\n",
       " 'Temp Range']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns = [col for col in data.columns if col not in target_columns]\n",
    "\n",
    "# Handle missing values if any\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "for col in feature_columns:\n",
    "    if data[col].dtype == \"object\":\n",
    "        data[col] = label_encoder.fit_transform(data[col])\n",
    "\n",
    "feature_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af2c4e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Disease</th>\n",
       "      <th>Prescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Common Cold</td>\n",
       "      <td>Rest, Plenty of Fluids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allergic Rhinitis</td>\n",
       "      <td>Antihistamines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tonsillitis</td>\n",
       "      <td>Analgesics, Antibiotics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Influenza</td>\n",
       "      <td>Bed Rest, Antiviral Medication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coronary Artery Disease</td>\n",
       "      <td>Aspirin, Statins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Pharyngitis</td>\n",
       "      <td>Analgesics, Antibiotics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Allergic Rhinitis</td>\n",
       "      <td>Antihistamines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Asthma</td>\n",
       "      <td>Bronchodilators, Inhalers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Upper Respiratory Infection</td>\n",
       "      <td>Rest, Fluids, Cough Syrup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Common Cold</td>\n",
       "      <td>Rest, Plenty of Fluids</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Disease                    Prescription\n",
       "0                   Common Cold          Rest, Plenty of Fluids\n",
       "1             Allergic Rhinitis                  Antihistamines\n",
       "2                   Tonsillitis         Analgesics, Antibiotics\n",
       "3                     Influenza  Bed Rest, Antiviral Medication\n",
       "4       Coronary Artery Disease                Aspirin, Statins\n",
       "..                          ...                             ...\n",
       "82                  Pharyngitis         Analgesics, Antibiotics\n",
       "83            Allergic Rhinitis                  Antihistamines\n",
       "84                       Asthma       Bronchodilators, Inhalers\n",
       "85  Upper Respiratory Infection       Rest, Fluids, Cough Syrup\n",
       "86                  Common Cold          Rest, Plenty of Fluids\n",
       "\n",
       "[87 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "data[feature_columns] = scaler.fit_transform(data[feature_columns])\n",
    "\n",
    "# Split data into input features and target variables\n",
    "X = data[feature_columns]\n",
    "y = data[target_columns]\n",
    "\n",
    "\n",
    "\n",
    "X\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70f43eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Model Architecture\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(len(feature_columns),)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(len(target_columns), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bc014f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (78, 2) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Convert target variable to integer values\u001b[39;00m\n\u001b[0;32m     11\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[1;32m---> 12\u001b[0m y_train_encoded \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(y_train)\n\u001b[0;32m     13\u001b[0m y_test_encoded \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mtransform(y_test)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Convert integer labels to one-hot encoded vectors\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115\u001b[0m, in \u001b[0;36mLabelEncoder.fit_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit label encoder and return encoded labels.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m        Encoded labels.\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     y \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_, y \u001b[38;5;241m=\u001b[39m _unique(y, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1156\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, warn)\u001b[0m\n\u001b[0;32m   1147\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1148\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1149\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected. Please change the shape of y to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1153\u001b[0m         )\n\u001b[0;32m   1154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mravel(y)\n\u001b[1;32m-> 1156\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1157\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[0;32m   1158\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (78, 2) instead."
     ]
    }
   ],
   "source": [
    "X_test\n",
    "y_train\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Model Training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Convert target variable to integer values\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert integer labels to one-hot encoded vectors\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_train_one_hot = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_test_one_hot = to_categorical(y_test_encoded, num_classes=num_classes)\n",
    "\n",
    "# Step 3: Model Training\n",
    "model.fit(X_train, y_train_one_hot, epochs=10, batch_size=32, validation_data=(X_test, y_test_one_hot))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad45406",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Model Evaluation\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "# Step 5: Model Fine-tuning\n",
    "# Adjust hyperparameters, modify architecture, or introduce regularization techniques\n",
    "\n",
    "\n",
    "# Step 6: Model Deployment\n",
    "model.save(\"dnn_model.h5\")  # Save the trained model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a9d2aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\AppData\\Local\\Temp\\ipykernel_26708\\767643508.py:17: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if X[column].dtype == np.object:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m X\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X[column]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject:\n\u001b[0;32m     18\u001b[0m         X[column] \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit_transform(X[column])\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# One-hot encoding the target variable\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    300\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;66;03m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "data = pd.read_csv('your_dataset.csv')\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y = data[['Disease', 'Prescription']]\n",
    "\n",
    "# Encoding categorical features\n",
    "encoder = LabelEncoder()\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == np.object:\n",
    "        X[column] = encoder.fit_transform(X[column])\n",
    "\n",
    "# One-hot encoding the target variable\n",
    "y_encoded = pd.get_dummies(y)\n",
    "\n",
    "# Step 2: Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=X.shape[1]))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(y_encoded.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Model Training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.1, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Step 4: Model Evaluation\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "515cc646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\AppData\\Local\\Temp\\ipykernel_26708\\201610629.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[column] = encoder.fit_transform(X[column])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 20.4989 - accuracy: 0.0000e+00 - val_loss: 13.6358 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 11.7446 - accuracy: 0.0042 - val_loss: 9.6113 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 9.5617 - accuracy: 0.0792 - val_loss: 8.8559 - val_accuracy: 0.2963\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 9.1948 - accuracy: 0.1750 - val_loss: 8.1741 - val_accuracy: 0.1481\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.4847 - accuracy: 0.0625 - val_loss: 7.6736 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.0520 - accuracy: 0.0500 - val_loss: 7.1713 - val_accuracy: 0.1111\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.8169 - accuracy: 0.1208 - val_loss: 6.9833 - val_accuracy: 0.1111\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.7380 - accuracy: 0.1167 - val_loss: 6.8925 - val_accuracy: 0.1111\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.7720 - accuracy: 0.1083 - val_loss: 7.0642 - val_accuracy: 0.1852\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.8432 - accuracy: 0.2292 - val_loss: 7.2358 - val_accuracy: 0.2222\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7.2358 - accuracy: 0.2222\n",
      "Test Loss: 7.235806941986084\n",
      "Test Accuracy: 0.2222222238779068\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "data = pd.read_csv('data.csv')\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y = data[['Disease', 'Prescription']]\n",
    "\n",
    "# Encoding categorical features\n",
    "encoder = LabelEncoder()\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == object:\n",
    "        X[column] = encoder.fit_transform(X[column])\n",
    "\n",
    "# One-hot encoding the target variable\n",
    "y_encoded = pd.get_dummies(y)\n",
    "\n",
    "# Step 2: Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=X.shape[1]))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(y_encoded.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Model Training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.1, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Step 4: Model Evaluation\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6506254",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d2f650c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\AppData\\Local\\Temp\\ipykernel_26708\\2070597821.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[column] = encoder.fit_transform(X[column])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "8/8 [==============================] - 1s 26ms/step - loss: 6.7513 - accuracy: 0.2083 - val_loss: 6.3081 - val_accuracy: 0.2963\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6.4185 - accuracy: 0.2917 - val_loss: 5.9756 - val_accuracy: 0.4074\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 6.2189 - accuracy: 0.1542 - val_loss: 5.7254 - val_accuracy: 0.1852\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6.0906 - accuracy: 0.1125 - val_loss: 5.5240 - val_accuracy: 0.1852\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5.9862 - accuracy: 0.1125 - val_loss: 5.3453 - val_accuracy: 0.1852\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 5.9016 - accuracy: 0.1583 - val_loss: 5.1945 - val_accuracy: 0.2222\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 5.8622 - accuracy: 0.2167 - val_loss: 5.0886 - val_accuracy: 0.1852\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 5.8536 - accuracy: 0.1042 - val_loss: 5.0227 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 5.8842 - accuracy: 0.0292 - val_loss: 5.0010 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 5.9552 - accuracy: 0.0583 - val_loss: 4.9914 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 6.1352 - accuracy: 0.0292 - val_loss: 5.0365 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 6.2365 - accuracy: 0.0458 - val_loss: 5.0405 - val_accuracy: 0.0741\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 6.3831 - accuracy: 0.1500 - val_loss: 5.0509 - val_accuracy: 0.1111\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6.5283 - accuracy: 0.1583 - val_loss: 5.0736 - val_accuracy: 0.1111\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.7645 - accuracy: 0.1542 - val_loss: 5.0491 - val_accuracy: 0.1111\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6.9653 - accuracy: 0.1417 - val_loss: 5.0897 - val_accuracy: 0.1111\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 7.2288 - accuracy: 0.1375 - val_loss: 5.0015 - val_accuracy: 0.1111\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 7.5602 - accuracy: 0.1625 - val_loss: 5.0618 - val_accuracy: 0.1111\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 7.8175 - accuracy: 0.1542 - val_loss: 5.2059 - val_accuracy: 0.1111\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 8.4515 - accuracy: 0.1750 - val_loss: 5.3417 - val_accuracy: 0.1111\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 9.1802 - accuracy: 0.1833 - val_loss: 5.3566 - val_accuracy: 0.1111\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 9.8469 - accuracy: 0.1542 - val_loss: 5.4422 - val_accuracy: 0.1111\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 10.4264 - accuracy: 0.1875 - val_loss: 5.5586 - val_accuracy: 0.1111\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 11.6267 - accuracy: 0.1500 - val_loss: 5.7540 - val_accuracy: 0.1111\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12.3321 - accuracy: 0.0833 - val_loss: 5.8220 - val_accuracy: 0.1481\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 13.3118 - accuracy: 0.1208 - val_loss: 5.9100 - val_accuracy: 0.1111\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 14.6845 - accuracy: 0.1708 - val_loss: 6.3693 - val_accuracy: 0.1111\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 15.7028 - accuracy: 0.2125 - val_loss: 5.8528 - val_accuracy: 0.1111\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 17.0715 - accuracy: 0.0583 - val_loss: 5.8870 - val_accuracy: 0.1111\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 18.2242 - accuracy: 0.1625 - val_loss: 5.8636 - val_accuracy: 0.0370\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5.8636 - accuracy: 0.0370\n",
      "Test Loss: 5.8635640144348145\n",
      "Test Accuracy: 0.03703703731298447\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "data = pd.read_csv('data.csv')\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y = data[['Disease', 'Prescription']]\n",
    "\n",
    "# Encoding categorical features\n",
    "encoder = LabelEncoder()\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == object:\n",
    "        X[column] = encoder.fit_transform(X[column])\n",
    "\n",
    "# One-hot encoding the target variable\n",
    "y_encoded = pd.get_dummies(y)\n",
    "\n",
    "# Define priority percentages for each column\n",
    "priority_percentages = [0.2, 0.2, 0.2, 0.15, 0.10, 0.40, 0.15, 0.15, 0.05, 0.15, 0.15, 0.15]\n",
    "\n",
    "# Normalize priority percentages\n",
    "priority_weights = np.array(priority_percentages) / np.sum(priority_percentages)\n",
    "\n",
    "# Multiply features with priority weights\n",
    "weighted_X = X * priority_weights\n",
    "\n",
    "# Step 2: Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=weighted_X.shape[1]))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(y_encoded.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Model Training\n",
    "X_train, X_test, y_train, y_test = train_test_split(weighted_X, y_encoded, test_size=0.1, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Step 4: Model Evaluation\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03271a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\AppData\\Local\\Temp\\ipykernel_26708\\1071816258.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[column] = encoder.fit_transform(X[column])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9629629629629629\n",
      "Please provide the following information:\n",
      "Location: Ernakulam\n",
      "Gender: Male\n",
      "Age: 21\n",
      "BMI: 25\n",
      "Allergy: None\n",
      "Symptoms: cough,fever\n",
      "Duration of Symptoms: 4\n",
      "Current Disease Info: None\n",
      "Occupation: Engineer\n",
      "Smoker (yes/no): no\n",
      "BP Range: 120/80\n",
      "Temp Range: 37.5°C\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (1, 12) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m input_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m: [location],\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m'\u001b[39m: [gender],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp Range\u001b[39m\u001b[38;5;124m'\u001b[39m: [temp_range]\n\u001b[0;32m     77\u001b[0m })\n\u001b[0;32m     79\u001b[0m X_new \u001b[38;5;241m=\u001b[39m input_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBMI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllergy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymptoms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuration of Symptoms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCurrent Disease Info\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOccupation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSmoker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBP Range\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp Range\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m---> 80\u001b[0m X_new_encoded \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(X_new)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     81\u001b[0m priority_weights_new \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(priority_percentages) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(priority_percentages)\n\u001b[0;32m     82\u001b[0m weighted_X_new \u001b[38;5;241m=\u001b[39m X_new_encoded \u001b[38;5;241m*\u001b[39m priority_weights_new\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"Transform labels to normalized encoding.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    Labels as normalized encodings.\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 133\u001b[0m y \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# transform of empty array is empty array\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1156\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, warn)\u001b[0m\n\u001b[0;32m   1147\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1148\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1149\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected. Please change the shape of y to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1153\u001b[0m         )\n\u001b[0;32m   1154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mravel(y)\n\u001b[1;32m-> 1156\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1157\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[0;32m   1158\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (1, 12) instead."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "data = pd.read_csv('data.csv')\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y = data[['Disease', 'Prescription']]\n",
    "\n",
    "# Encoding categorical features\n",
    "encoder = LabelEncoder()\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == object:\n",
    "        X[column] = encoder.fit_transform(X[column])\n",
    "\n",
    "# One-hot encoding the target variable\n",
    "y_encoded = pd.get_dummies(y)\n",
    "\n",
    "# Define priority percentages for each column\n",
    "priority_percentages = [0.2, 0.2, 0.2, 0.15, 0.10, 0.40, 0.15, 0.15, 0.05, 0.15, 0.15, 0.15]\n",
    "\n",
    "# Normalize priority percentages\n",
    "priority_weights = np.array(priority_percentages) / np.sum(priority_percentages)\n",
    "\n",
    "# Multiply features with priority weights\n",
    "weighted_X = X * priority_weights\n",
    "\n",
    "# Step 2: Define and train the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(weighted_X, y_encoded, test_size=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Make predictions and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Step 3: Save the model\n",
    "joblib.dump(model, 'random_forest_model.pkl')\n",
    "\n",
    "# Step 4: Load the model\n",
    "loaded_model = joblib.load('random_forest_model.pkl')\n",
    "\n",
    "# Step 5: Prompt user to input new data\n",
    "print(\"Please provide the following information:\")\n",
    "location = input(\"Location: \")\n",
    "gender = input(\"Gender: \")\n",
    "age = float(input(\"Age: \"))\n",
    "bmi = float(input(\"BMI: \"))\n",
    "allergy = input(\"Allergy: \")\n",
    "symptoms = input(\"Symptoms: \")\n",
    "duration = float(input(\"Duration of Symptoms: \"))\n",
    "disease_info = input(\"Current Disease Info: \")\n",
    "occupation = input(\"Occupation: \")\n",
    "smoker = input(\"Smoker (yes/no): \")\n",
    "bp_range = input(\"BP Range: \")\n",
    "temp_range = input(\"Temp Range: \")\n",
    "\n",
    "# Preprocess the new data\n",
    "input_data = pd.DataFrame({\n",
    "    'Location': [location],\n",
    "    'Gender': [gender],\n",
    "    'Age': [age],\n",
    "    'BMI': [bmi],\n",
    "    'Allergy': [allergy],\n",
    "    'Symptoms': [symptoms],\n",
    "    'Duration of Symptoms': [duration],\n",
    "    'Current Disease Info': [disease_info],\n",
    "    'Occupation': [occupation],\n",
    "    'Smoker': [smoker],\n",
    "    'BP Range': [bp_range],\n",
    "    'Temp Range': [temp_range]\n",
    "})\n",
    "\n",
    "X_new = input_data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "X_new_encoded = encoder.transform(X_new).toarray()\n",
    "priority_weights_new = np.array(priority_percentages) / np.sum(priority_percentages)\n",
    "weighted_X_new = X_new_encoded * priority_weights_new\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = loaded_model.predict(weighted_X_new)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a004267e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (267,147) (12,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m priority_percentages \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Multiply features with priority weights\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m weighted_X \u001b[38;5;241m=\u001b[39m X_encoded \u001b[38;5;241m*\u001b[39m priority_percentages\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Step 2: Define and train the Random Forest model\u001b[39;00m\n\u001b[0;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (267,147) (12,) "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "data = pd.read_csv('data.csv')\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y = data[['Disease', 'Prescription']]\n",
    "\n",
    "# Encoding categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_encoded = encoder.fit_transform(X).toarray()\n",
    "\n",
    "# One-hot encoding the target variable\n",
    "y_encoded = pd.get_dummies(y)\n",
    "\n",
    "# Define priority percentages for each column\n",
    "priority_percentages = [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "\n",
    "# Multiply features with priority weights\n",
    "weighted_X = X_encoded * priority_percentages\n",
    "\n",
    "# Step 2: Define and train the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(weighted_X, y_encoded, test_size=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Save the model\n",
    "joblib.dump(model, 'random_forest_model.pkl')\n",
    "\n",
    "# Step 4: Load the model\n",
    "loaded_model = joblib.load('random_forest_model.pkl')\n",
    "\n",
    "# Step 5: Test the model on new data\n",
    "new_data = pd.read_csv('new_data.csv')  # Load your new data from a CSV file\n",
    "X_new = new_data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "\n",
    "# Preprocess the new data\n",
    "X_new_encoded = encoder.transform(X_new).toarray()\n",
    "priority_weights_new = np.array(priority_percentages) / np.sum(priority_percentages)\n",
    "weighted_X_new = X_new_encoded * priority_weights_new\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = loaded_model.predict(weighted_X_new)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a74c50c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (267,147) (12,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m priority_percentages \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Multiply features with priority weights\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m weighted_X \u001b[38;5;241m=\u001b[39m X_encoded \u001b[38;5;241m*\u001b[39m priority_percentages\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Step 2: Define and train the Random Forest model\u001b[39;00m\n\u001b[0;32m     26\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (267,147) (12,) "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "data = pd.read_csv('data.csv')\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y = data[['Disease', 'Prescription']]\n",
    "\n",
    "# Encoding categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_encoded = encoder.fit_transform(X).toarray()\n",
    "\n",
    "# One-hot encoding the target variable\n",
    "y_encoded = pd.get_dummies(y)\n",
    "\n",
    "# Define priority percentages for each column\n",
    "priority_percentages = [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "\n",
    "# Multiply features with priority weights\n",
    "weighted_X = X_encoded * priority_percentages\n",
    "\n",
    "# Step 2: Define and train the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(weighted_X, y_encoded)\n",
    "\n",
    "# Step 3: Save the model\n",
    "joblib.dump(model, 'random_forest_model.pkl')\n",
    "\n",
    "# Step 4: Load the model\n",
    "loaded_model = joblib.load('random_forest_model.pkl')\n",
    "\n",
    "# Step 5: Prompt user to input new data\n",
    "print(\"Please provide the following information:\")\n",
    "location = input(\"Location: \")\n",
    "gender = input(\"Gender: \")\n",
    "age = float(input(\"Age: \"))\n",
    "bmi = float(input(\"BMI: \"))\n",
    "allergy = input(\"Allergy: \")\n",
    "symptoms = input(\"Symptoms: \")\n",
    "duration = float(input(\"Duration of Symptoms: \"))\n",
    "disease_info = input(\"Current Disease Info: \")\n",
    "occupation = input(\"Occupation: \")\n",
    "smoker = input(\"Smoker (yes/no): \")\n",
    "bp_range = input(\"BP Range: \")\n",
    "temp_range = input(\"Temp Range: \")\n",
    "\n",
    "# Preprocess the new data\n",
    "input_data = pd.DataFrame({\n",
    "    'Location': [location],\n",
    "    'Gender': [gender],\n",
    "    'Age': [age],\n",
    "    'BMI': [bmi],\n",
    "    'Allergy': [allergy],\n",
    "    'Symptoms': [symptoms],\n",
    "    'Duration of Symptoms': [duration],\n",
    "    'Current Disease Info': [disease_info],\n",
    "    'Occupation': [occupation],\n",
    "    'Smoker': [smoker],\n",
    "    'BP Range': [bp_range],\n",
    "    'Temp Range': [temp_range]\n",
    "})\n",
    "\n",
    "X_new = input_data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "X_new_encoded = encoder.transform(X_new).toarray()\n",
    "priority_weights_new = np.array(priority_percentages) / np.sum(priority_percentages)\n",
    "weighted_X_new = X_new_encoded * priority_weights_new\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = loaded_model.predict(weighted_X_new)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3b35041",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (267,147) (12,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m priority_percentages \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Multiply features with priority weights\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m weighted_X \u001b[38;5;241m=\u001b[39m X_encoded \u001b[38;5;241m*\u001b[39m priority_percentages\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Step 2: Define and train the Random Forest model\u001b[39;00m\n\u001b[0;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (267,147) (12,) "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "data = pd.read_csv('data.csv')\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y = data['Disease']\n",
    "\n",
    "# Encoding categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_encoded = encoder.fit_transform(X).toarray()\n",
    "\n",
    "# One-hot encoding the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Define priority percentages for each column\n",
    "priority_percentages = [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "\n",
    "# Multiply features with priority weights\n",
    "weighted_X = X_encoded * priority_percentages\n",
    "\n",
    "# Step 2: Define and train the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(weighted_X, y_encoded, test_size=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Save the model\n",
    "joblib.dump(model, 'random_forest_model.pkl')\n",
    "\n",
    "# Step 4: Load the model\n",
    "loaded_model = joblib.load('random_forest_model.pkl')\n",
    "\n",
    "# Step 5: Test the model on new data\n",
    "new_data = pd.read_csv('new_data.csv')  # Load your new data from a CSV file\n",
    "X_new = new_data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "\n",
    "# Preprocess the new data\n",
    "X_new_encoded = encoder.transform(X_new).toarray()\n",
    "priority_weights_new = np.array(priority_percentages) / np.sum(priority_percentages)\n",
    "weighted_X_new = X_new_encoded * priority_weights_new\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = loaded_model.predict(weighted_X_new)\n",
    "\n",
    "# Convert predictions back to disease labels\n",
    "predicted_diseases = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "# Print the predictions\n",
    "print(predicted_diseases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8485892",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (267,147) (12,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [47]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m priority_percentages \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Multiply features with priority weights\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m weighted_X \u001b[38;5;241m=\u001b[39m X_encoded \u001b[38;5;241m*\u001b[39m priority_percentages\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Step 2: Define and train the Random Forest model\u001b[39;00m\n\u001b[0;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (267,147) (12,) "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Handling Missing Values (if any)\n",
    "data.fillna(value='NA', inplace=True)\n",
    "\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y = data[['Disease', 'Prescription']]\n",
    "\n",
    "# Encoding categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_encoded = encoder.fit_transform(X).toarray()\n",
    "\n",
    "# One-hot encoding the target variable\n",
    "y_encoded = pd.get_dummies(y)\n",
    "\n",
    "# Define priority percentages for each column\n",
    "priority_percentages = [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "\n",
    "# Multiply features with priority weights\n",
    "weighted_X = X_encoded * priority_percentages\n",
    "\n",
    "# Step 2: Define and train the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(weighted_X, y_encoded)\n",
    "\n",
    "# Step 3: Save the model\n",
    "joblib.dump(model, 'random_forest_model.pkl')\n",
    "\n",
    "# Step 4: Load the model\n",
    "loaded_model = joblib.load('random_forest_model.pkl')\n",
    "\n",
    "# Step 5: Prompt user to input new data\n",
    "print(\"Please provide the following information:\")\n",
    "location = input(\"Location: \")\n",
    "gender = input(\"Gender: \")\n",
    "age = float(input(\"Age: \"))\n",
    "bmi = float(input(\"BMI: \"))\n",
    "allergy = input(\"Allergy: \")\n",
    "symptoms = input(\"Symptoms: \")\n",
    "duration = float(input(\"Duration of Symptoms: \"))\n",
    "disease_info = input(\"Current Disease Info: \")\n",
    "occupation = input(\"Occupation: \")\n",
    "smoker = input(\"Smoker (yes/no): \")\n",
    "smoker = 1 if smoker.lower() == \"yes\" else 0\n",
    "bp_range = input(\"BP Range: \")\n",
    "temp_range = input(\"Temp Range: \")\n",
    "\n",
    "# Preprocess the new data\n",
    "input_data = pd.DataFrame({\n",
    "    'Location': [location],\n",
    "    'Gender': [gender],\n",
    "    'Age': [age],\n",
    "    'BMI': [bmi],\n",
    "    'Allergy': [allergy],\n",
    "    'Symptoms': [symptoms],\n",
    "    'Duration of Symptoms': [duration],\n",
    "    'Current Disease Info': [disease_info],\n",
    "    'Occupation': [occupation],\n",
    "    'Smoker': [smoker],\n",
    "    'BP Range': [bp_range],\n",
    "    'Temp Range': [temp_range]\n",
    "})\n",
    "\n",
    "X_new = input_data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "X_new_encoded = encoder.transform(X_new).toarray()\n",
    "priority_weights_new = np.array(priority_percentages) / np.sum(priority_percentages)\n",
    "weighted_X_new = X_new_encoded.reshape(1, -1) * priority_weights_new.reshape(1, -1)\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = loaded_model.predict(weighted_X_new)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5fef5be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide the following information:\n",
      "Location: Ernakulam\n",
      "Gender: Male\n",
      "Age: 21\n",
      "BMI: 25\n",
      "Allergy: None\n",
      "Symptoms: mild fever,headache\n",
      "Duration of Symptoms: 4\n",
      "Current Disease Info: None\n",
      "Occupation: Engineer\n",
      "Smoker (yes/no): no\n",
      "BP Range: 120/80\n",
      "Temp Range: 37.2°C\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [48]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m input_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m: [location],\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m'\u001b[39m: [gender],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp Range\u001b[39m\u001b[38;5;124m'\u001b[39m: [temp_range]\n\u001b[0;32m     69\u001b[0m })\n\u001b[0;32m     71\u001b[0m X_new \u001b[38;5;241m=\u001b[39m input_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBMI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllergy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymptoms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuration of Symptoms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCurrent Disease Info\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOccupation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSmoker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBP Range\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp Range\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m---> 72\u001b[0m X_new_encoded \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(X_new)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     73\u001b[0m priority_weights_new \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(priority_percentages) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(priority_percentages)\n\u001b[0;32m     74\u001b[0m weighted_X_new \u001b[38;5;241m=\u001b[39m X_new_encoded\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m priority_weights_new\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:882\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# validation of X happens in _check_X called by _transform\u001b[39;00m\n\u001b[0;32m    878\u001b[0m warn_on_unknown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfrequent_if_exist\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    881\u001b[0m }\n\u001b[1;32m--> 882\u001b[0m X_int, X_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(\n\u001b[0;32m    883\u001b[0m     X,\n\u001b[0;32m    884\u001b[0m     handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown,\n\u001b[0;32m    885\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    886\u001b[0m     warn_on_unknown\u001b[38;5;241m=\u001b[39mwarn_on_unknown,\n\u001b[0;32m    887\u001b[0m )\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_infrequent_categories(X_int, X_mask)\n\u001b[0;32m    890\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X_int\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:152\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[1;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_features):\n\u001b[0;32m    151\u001b[0m     Xi \u001b[38;5;241m=\u001b[39m X_list[i]\n\u001b[1;32m--> 152\u001b[0m     diff, valid_mask \u001b[38;5;241m=\u001b[39m _check_unknown(Xi, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories_[i], return_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(valid_mask):\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m handle_unknown \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_encode.py:303\u001b[0m, in \u001b[0;36m_check_unknown\u001b[1;34m(values, known_values, return_mask)\u001b[0m\n\u001b[0;32m    300\u001b[0m         valid_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(values), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;66;03m# check for nans in the known_values\u001b[39;00m\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(known_values)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    304\u001b[0m     diff_is_nan \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(diff)\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff_is_nan\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;66;03m# removes nan from valid_mask\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Handling Missing Values (if any)\n",
    "data.fillna(value='NA', inplace=True)\n",
    "\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y = data[['Disease', 'Prescription']]\n",
    "\n",
    "# Encoding categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "X_encoded = encoder.fit_transform(X).toarray()\n",
    "\n",
    "# One-hot encoding the target variable\n",
    "y_encoded = pd.get_dummies(y)\n",
    "\n",
    "# Define priority percentages for each column\n",
    "priority_percentages = [0.2] * X_encoded.shape[1]\n",
    "\n",
    "# Multiply features with priority weights\n",
    "weighted_X = X_encoded * priority_percentages\n",
    "\n",
    "# Step 2: Define and train the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(weighted_X, y_encoded)\n",
    "\n",
    "# Step 3: Save the model\n",
    "joblib.dump(model, 'random_forest_model.pkl')\n",
    "\n",
    "# Step 4: Load the model\n",
    "loaded_model = joblib.load('random_forest_model.pkl')\n",
    "\n",
    "# Step 5: Prompt user to input new data\n",
    "print(\"Please provide the following information:\")\n",
    "location = input(\"Location: \")\n",
    "gender = input(\"Gender: \")\n",
    "age = float(input(\"Age: \"))\n",
    "bmi = float(input(\"BMI: \"))\n",
    "allergy = input(\"Allergy: \")\n",
    "symptoms = input(\"Symptoms: \")\n",
    "duration = float(input(\"Duration of Symptoms: \"))\n",
    "disease_info = input(\"Current Disease Info: \")\n",
    "occupation = input(\"Occupation: \")\n",
    "smoker = input(\"Smoker (yes/no): \")\n",
    "smoker = 1 if smoker.lower() == \"yes\" else 0\n",
    "bp_range = input(\"BP Range: \")\n",
    "temp_range = input(\"Temp Range: \")\n",
    "\n",
    "# Preprocess the new data\n",
    "input_data = pd.DataFrame({\n",
    "    'Location': [location],\n",
    "    'Gender': [gender],\n",
    "    'Age': [age],\n",
    "    'BMI': [bmi],\n",
    "    'Allergy': [allergy],\n",
    "    'Symptoms': [symptoms],\n",
    "    'Duration of Symptoms': [duration],\n",
    "    'Current Disease Info': [disease_info],\n",
    "    'Occupation': [occupation],\n",
    "    'Smoker': [smoker],\n",
    "    'BP Range': [bp_range],\n",
    "    'Temp Range': [temp_range]\n",
    "})\n",
    "\n",
    "X_new = input_data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "X_new_encoded = encoder.transform(X_new).toarray()\n",
    "priority_weights_new = np.array(priority_percentages) / np.sum(priority_percentages)\n",
    "weighted_X_new = X_new_encoded.reshape(1, -1) * priority_weights_new\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = loaded_model.predict(weighted_X_new)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d06a883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide the following information:\n",
      "Location: Ernakulam\n",
      "Gender: Male\n",
      "Age: 21\n",
      "BMI: 25\n",
      "Allergy: None\n",
      "Symptoms: mild fever,cold\n",
      "Duration of Symptoms: 4\n",
      "Current Disease Info: None\n",
      "Occupation: Engineer\n",
      "Smoker (yes/no): no\n",
      "BP Range: 120/80\n",
      "Temp Range: 37.0°C\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [49]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m input_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m: [location],\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m'\u001b[39m: [gender],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp Range\u001b[39m\u001b[38;5;124m'\u001b[39m: [temp_range]\n\u001b[0;32m     69\u001b[0m })\n\u001b[0;32m     71\u001b[0m X_new \u001b[38;5;241m=\u001b[39m input_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBMI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllergy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymptoms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuration of Symptoms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCurrent Disease Info\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOccupation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSmoker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBP Range\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp Range\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m---> 72\u001b[0m X_new_encoded \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(X_new)\n\u001b[0;32m     73\u001b[0m priority_weights_new \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(priority_percentages) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(priority_percentages)\n\u001b[0;32m     74\u001b[0m weighted_X_new \u001b[38;5;241m=\u001b[39m X_new_encoded \u001b[38;5;241m*\u001b[39m priority_weights_new\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:882\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# validation of X happens in _check_X called by _transform\u001b[39;00m\n\u001b[0;32m    878\u001b[0m warn_on_unknown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfrequent_if_exist\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    881\u001b[0m }\n\u001b[1;32m--> 882\u001b[0m X_int, X_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(\n\u001b[0;32m    883\u001b[0m     X,\n\u001b[0;32m    884\u001b[0m     handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown,\n\u001b[0;32m    885\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    886\u001b[0m     warn_on_unknown\u001b[38;5;241m=\u001b[39mwarn_on_unknown,\n\u001b[0;32m    887\u001b[0m )\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_infrequent_categories(X_int, X_mask)\n\u001b[0;32m    890\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X_int\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:152\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[1;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_features):\n\u001b[0;32m    151\u001b[0m     Xi \u001b[38;5;241m=\u001b[39m X_list[i]\n\u001b[1;32m--> 152\u001b[0m     diff, valid_mask \u001b[38;5;241m=\u001b[39m _check_unknown(Xi, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories_[i], return_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(valid_mask):\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m handle_unknown \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_encode.py:303\u001b[0m, in \u001b[0;36m_check_unknown\u001b[1;34m(values, known_values, return_mask)\u001b[0m\n\u001b[0;32m    300\u001b[0m         valid_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(values), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;66;03m# check for nans in the known_values\u001b[39;00m\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(known_values)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    304\u001b[0m     diff_is_nan \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(diff)\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff_is_nan\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;66;03m# removes nan from valid_mask\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Handling Missing Values (if any)\n",
    "data.fillna(value='NA', inplace=True)\n",
    "\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y = data[['Disease', 'Prescription']]\n",
    "\n",
    "# Encoding categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False, dtype=int)  # Set sparse=False and dtype=int\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# One-hot encoding the target variable\n",
    "y_encoded = pd.get_dummies(y)\n",
    "\n",
    "# Define priority percentages for each column\n",
    "priority_percentages = [0.2] * X_encoded.shape[1]\n",
    "\n",
    "# Multiply features with priority weights\n",
    "weighted_X = X_encoded * priority_percentages\n",
    "\n",
    "# Step 2: Define and train the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(weighted_X, y_encoded)\n",
    "\n",
    "# Step 3: Save the model\n",
    "joblib.dump(model, 'random_forest_model.pkl')\n",
    "\n",
    "# Step 4: Load the model\n",
    "loaded_model = joblib.load('random_forest_model.pkl')\n",
    "\n",
    "# Step 5: Prompt user to input new data\n",
    "print(\"Please provide the following information:\")\n",
    "location = input(\"Location: \")\n",
    "gender = input(\"Gender: \")\n",
    "age = float(input(\"Age: \"))\n",
    "bmi = float(input(\"BMI: \"))\n",
    "allergy = input(\"Allergy: \")\n",
    "symptoms = input(\"Symptoms: \")\n",
    "duration = float(input(\"Duration of Symptoms: \"))\n",
    "disease_info = input(\"Current Disease Info: \")\n",
    "occupation = input(\"Occupation: \")\n",
    "smoker = input(\"Smoker (yes/no): \")\n",
    "smoker = 1 if smoker.lower() == \"yes\" else 0\n",
    "bp_range = input(\"BP Range: \")\n",
    "temp_range = input(\"Temp Range: \")\n",
    "\n",
    "# Preprocess the new data\n",
    "input_data = pd.DataFrame({\n",
    "    'Location': [location],\n",
    "    'Gender': [gender],\n",
    "    'Age': [age],\n",
    "    'BMI': [bmi],\n",
    "    'Allergy': [allergy],\n",
    "    'Symptoms': [symptoms],\n",
    "    'Duration of Symptoms': [duration],\n",
    "    'Current Disease Info': [disease_info],\n",
    "    'Occupation': [occupation],\n",
    "    'Smoker': [smoker],\n",
    "    'BP Range': [bp_range],\n",
    "    'Temp Range': [temp_range]\n",
    "})\n",
    "\n",
    "X_new = input_data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "X_new_encoded = encoder.transform(X_new)\n",
    "priority_weights_new = np.array(priority_percentages) / np.sum(priority_percentages)\n",
    "weighted_X_new = X_new_encoded * priority_weights_new\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = loaded_model.predict(weighted_X_new)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d82cfb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide the following information:\n",
      "Location: Ernakulam\n",
      "Gender: Male\n",
      "Age: 21\n",
      "BMI: 25\n",
      "Allergy: None\n",
      "Symptoms: mild cold,fever\n",
      "Duration of Symptoms: 4\n",
      "Current Disease Info: None\n",
      "Occupation: Engineer\n",
      "Smoker (yes/no): no\n",
      "BP Range: 37.0°C\n",
      "Temp Range: 37.0°C\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m input_data\u001b[38;5;241m.\u001b[39mfillna(value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNA\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     74\u001b[0m X_new \u001b[38;5;241m=\u001b[39m input_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBMI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllergy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymptoms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuration of Symptoms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCurrent Disease Info\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOccupation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSmoker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBP Range\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp Range\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m---> 75\u001b[0m X_new_encoded \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(X_new)\n\u001b[0;32m     76\u001b[0m priority_weights_new \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(priority_percentages) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(priority_percentages)\n\u001b[0;32m     77\u001b[0m weighted_X_new \u001b[38;5;241m=\u001b[39m X_new_encoded \u001b[38;5;241m*\u001b[39m priority_weights_new\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:882\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# validation of X happens in _check_X called by _transform\u001b[39;00m\n\u001b[0;32m    878\u001b[0m warn_on_unknown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfrequent_if_exist\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    881\u001b[0m }\n\u001b[1;32m--> 882\u001b[0m X_int, X_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(\n\u001b[0;32m    883\u001b[0m     X,\n\u001b[0;32m    884\u001b[0m     handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown,\n\u001b[0;32m    885\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    886\u001b[0m     warn_on_unknown\u001b[38;5;241m=\u001b[39mwarn_on_unknown,\n\u001b[0;32m    887\u001b[0m )\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_infrequent_categories(X_int, X_mask)\n\u001b[0;32m    890\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X_int\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:152\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[1;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_features):\n\u001b[0;32m    151\u001b[0m     Xi \u001b[38;5;241m=\u001b[39m X_list[i]\n\u001b[1;32m--> 152\u001b[0m     diff, valid_mask \u001b[38;5;241m=\u001b[39m _check_unknown(Xi, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories_[i], return_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(valid_mask):\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m handle_unknown \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_encode.py:303\u001b[0m, in \u001b[0;36m_check_unknown\u001b[1;34m(values, known_values, return_mask)\u001b[0m\n\u001b[0;32m    300\u001b[0m         valid_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(values), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;66;03m# check for nans in the known_values\u001b[39;00m\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(known_values)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    304\u001b[0m     diff_is_nan \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(diff)\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff_is_nan\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;66;03m# removes nan from valid_mask\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Handling Missing Values (if any)\n",
    "data.fillna(value='NA', inplace=True)\n",
    "\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y = data[['Disease', 'Prescription']]\n",
    "\n",
    "# Encoding categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False, dtype=int)  # Set sparse=False and dtype=int\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# One-hot encoding the target variable\n",
    "y_encoded = pd.get_dummies(y)\n",
    "\n",
    "# Define priority percentages for each column\n",
    "priority_percentages = [0.2] * X_encoded.shape[1]\n",
    "\n",
    "# Multiply features with priority weights\n",
    "weighted_X = X_encoded * priority_percentages\n",
    "\n",
    "# Step 2: Define and train the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(weighted_X, y_encoded)\n",
    "\n",
    "# Step 3: Save the model\n",
    "joblib.dump(model, 'random_forest_model.pkl')\n",
    "\n",
    "# Step 4: Load the model\n",
    "loaded_model = joblib.load('random_forest_model.pkl')\n",
    "\n",
    "# Step 5: Prompt user to input new data\n",
    "print(\"Please provide the following information:\")\n",
    "location = input(\"Location: \")\n",
    "gender = input(\"Gender: \")\n",
    "age = float(input(\"Age: \"))\n",
    "bmi = float(input(\"BMI: \"))\n",
    "allergy = input(\"Allergy: \")\n",
    "symptoms = input(\"Symptoms: \")\n",
    "duration = float(input(\"Duration of Symptoms: \"))\n",
    "disease_info = input(\"Current Disease Info: \")\n",
    "occupation = input(\"Occupation: \")\n",
    "smoker = input(\"Smoker (yes/no): \")\n",
    "smoker = 1 if smoker.lower() == \"yes\" else 0\n",
    "bp_range = input(\"BP Range: \")\n",
    "temp_range = input(\"Temp Range: \")\n",
    "\n",
    "# Preprocess the new data\n",
    "input_data = pd.DataFrame({\n",
    "    'Location': [location],\n",
    "    'Gender': [gender],\n",
    "    'Age': [age],\n",
    "    'BMI': [bmi],\n",
    "    'Allergy': [allergy],\n",
    "    'Symptoms': [symptoms],\n",
    "    'Duration of Symptoms': [duration],\n",
    "    'Current Disease Info': [disease_info],\n",
    "    'Occupation': [occupation],\n",
    "    'Smoker': [smoker],\n",
    "    'BP Range': [bp_range],\n",
    "    'Temp Range': [temp_range]\n",
    "})\n",
    "\n",
    "# Replace NaN values with 'NA'\n",
    "input_data.fillna(value='NA', inplace=True)\n",
    "\n",
    "X_new = input_data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "X_new_encoded = encoder.transform(X_new)\n",
    "priority_weights_new = np.array(priority_percentages) / np.sum(priority_percentages)\n",
    "weighted_X_new = X_new_encoded * priority_weights_new\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = loaded_model.predict(weighted_X_new)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11fca8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide the following information:\n",
      "Location: Ernakulam\n",
      "Gender: Male\n",
      "Age: 35\n",
      "BMI: 23\n",
      "Allergy: None\n",
      "Symptoms: mild fever,cold\n",
      "Duration of Symptoms: 4\n",
      "Current Disease Info: None\n",
      "Occupation: Engineer\n",
      "Smoker (yes/no): no\n",
      "BP Range: 120/80\n",
      "Temp Range: 36.9°C\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m input_data\u001b[38;5;241m.\u001b[39mfillna(value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNA\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     74\u001b[0m X_new \u001b[38;5;241m=\u001b[39m input_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBMI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllergy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymptoms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuration of Symptoms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCurrent Disease Info\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOccupation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSmoker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBP Range\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp Range\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m---> 75\u001b[0m X_new_encoded \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(X_new)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Create priority weights for new data\u001b[39;00m\n\u001b[0;32m     78\u001b[0m priority_weights_new \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(priority_percentages) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(priority_percentages)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:882\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# validation of X happens in _check_X called by _transform\u001b[39;00m\n\u001b[0;32m    878\u001b[0m warn_on_unknown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfrequent_if_exist\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    881\u001b[0m }\n\u001b[1;32m--> 882\u001b[0m X_int, X_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(\n\u001b[0;32m    883\u001b[0m     X,\n\u001b[0;32m    884\u001b[0m     handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown,\n\u001b[0;32m    885\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    886\u001b[0m     warn_on_unknown\u001b[38;5;241m=\u001b[39mwarn_on_unknown,\n\u001b[0;32m    887\u001b[0m )\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_infrequent_categories(X_int, X_mask)\n\u001b[0;32m    890\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X_int\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:152\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[1;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_features):\n\u001b[0;32m    151\u001b[0m     Xi \u001b[38;5;241m=\u001b[39m X_list[i]\n\u001b[1;32m--> 152\u001b[0m     diff, valid_mask \u001b[38;5;241m=\u001b[39m _check_unknown(Xi, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories_[i], return_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(valid_mask):\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m handle_unknown \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_encode.py:303\u001b[0m, in \u001b[0;36m_check_unknown\u001b[1;34m(values, known_values, return_mask)\u001b[0m\n\u001b[0;32m    300\u001b[0m         valid_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(values), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;66;03m# check for nans in the known_values\u001b[39;00m\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(known_values)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    304\u001b[0m     diff_is_nan \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(diff)\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diff_is_nan\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;66;03m# removes nan from valid_mask\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Handling Missing Values (if any)\n",
    "data.fillna(value='NA', inplace=True)\n",
    "\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y = data[['Disease', 'Prescription']]\n",
    "\n",
    "# Encoding categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False, dtype=int)  # Set sparse=False and dtype=int\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# One-hot encoding the target variable\n",
    "y_encoded = pd.get_dummies(y)\n",
    "\n",
    "# Define priority percentages for each column\n",
    "priority_percentages = [0.2] * X_encoded.shape[1]\n",
    "\n",
    "# Multiply features with priority weights\n",
    "weighted_X = X_encoded * priority_percentages\n",
    "\n",
    "# Step 2: Define and train the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(weighted_X, y_encoded)\n",
    "\n",
    "# Step 3: Save the model\n",
    "joblib.dump(model, 'random_forest_model.pkl')\n",
    "\n",
    "# Step 4: Load the model\n",
    "loaded_model = joblib.load('random_forest_model.pkl')\n",
    "\n",
    "# Step 5: Prompt user to input new data\n",
    "print(\"Please provide the following information:\")\n",
    "location = input(\"Location: \")\n",
    "gender = input(\"Gender: \")\n",
    "age = float(input(\"Age: \"))\n",
    "bmi = float(input(\"BMI: \"))\n",
    "allergy = input(\"Allergy: \")\n",
    "symptoms = input(\"Symptoms: \")\n",
    "duration = float(input(\"Duration of Symptoms: \"))\n",
    "disease_info = input(\"Current Disease Info: \")\n",
    "occupation = input(\"Occupation: \")\n",
    "smoker = input(\"Smoker (yes/no): \")\n",
    "smoker = 1 if smoker.lower() == \"yes\" else 0\n",
    "bp_range = input(\"BP Range: \")\n",
    "temp_range = input(\"Temp Range: \")\n",
    "\n",
    "# Preprocess the new data\n",
    "input_data = pd.DataFrame({\n",
    "    'Location': [location],\n",
    "    'Gender': [gender],\n",
    "    'Age': [age],\n",
    "    'BMI': [bmi],\n",
    "    'Allergy': [allergy],\n",
    "    'Symptoms': [symptoms],\n",
    "    'Duration of Symptoms': [duration],\n",
    "    'Current Disease Info': [disease_info],\n",
    "    'Occupation': [occupation],\n",
    "    'Smoker': [smoker],\n",
    "    'BP Range': [bp_range],\n",
    "    'Temp Range': [temp_range]\n",
    "})\n",
    "\n",
    "# Replace NaN values with 'NA'\n",
    "input_data.fillna(value='NA', inplace=True)\n",
    "\n",
    "X_new = input_data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "X_new_encoded = encoder.transform(X_new)\n",
    "\n",
    "# Create priority weights for new data\n",
    "priority_weights_new = np.array(priority_percentages) / np.sum(priority_percentages)\n",
    "\n",
    "# Apply priority weights to new data\n",
    "weighted_X_new = X_new_encoded * priority_weights_new.reshape(1, -1)\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = loaded_model.predict(weighted_X_new)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "071f4f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide the following information:\n",
      "Location: Ernakulam\n",
      "Gender: Male\n",
      "Age: 21\n",
      "BMI: 25\n",
      "Allergy: None\n",
      "Symptoms: mild cold,fever\n",
      "Duration of Symptoms: 4\n",
      "Current Disease Info: None\n",
      "Occupation: Engineer\n",
      "Smoker (yes/no): no\n",
      "BP Range: 120/80\n",
      "Temp Range: 37.2°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- Disease\n",
      "- Prescription\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 12 features, but SimpleImputer is expecting 14 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [52]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m input_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m: [location],\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m'\u001b[39m: [gender],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp Range\u001b[39m\u001b[38;5;124m'\u001b[39m: [temp_range]\n\u001b[0;32m     70\u001b[0m })\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Handle missing values in new data\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m input_data_filled \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(imputer\u001b[38;5;241m.\u001b[39mtransform(input_data), columns\u001b[38;5;241m=\u001b[39minput_data\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     75\u001b[0m X_new \u001b[38;5;241m=\u001b[39m input_data_filled[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBMI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllergy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymptoms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuration of Symptoms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCurrent Disease Info\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOccupation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSmoker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBP Range\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp Range\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     76\u001b[0m X_new_encoded \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(X_new)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\impute\\_base.py:496\u001b[0m, in \u001b[0;36mSimpleImputer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m\"\"\"Impute all missing values in `X`.\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \n\u001b[0;32m    483\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;124;03m    `X` with imputed values.\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    494\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 496\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_input(X, in_fit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    497\u001b[0m statistics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatistics_\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m statistics\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\impute\\_base.py:304\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[1;34m(self, X, in_fit)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m new_ve \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ve\n\u001b[0;32m    306\u001b[0m _check_inputs_dtype(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing_values)\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\impute\\_base.py:287\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[1;34m(self, X, in_fit)\u001b[0m\n\u001b[0;32m    284\u001b[0m     force_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 287\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    288\u001b[0m         X,\n\u001b[0;32m    289\u001b[0m         reset\u001b[38;5;241m=\u001b[39min_fit,\n\u001b[0;32m    290\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    291\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    292\u001b[0m         force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m    293\u001b[0m         copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy,\n\u001b[0;32m    294\u001b[0m     )\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not convert\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:600\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    597\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 12 features, but SimpleImputer is expecting 14 features as input."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Handling Missing Values (if any)\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "data_filled = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "\n",
    "X = data_filled[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y = data_filled[['Disease', 'Prescription']]\n",
    "\n",
    "# Encoding categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False, dtype=int)  # Set sparse=False and dtype=int\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# One-hot encoding the target variable\n",
    "y_encoded = pd.get_dummies(y)\n",
    "\n",
    "# Define priority percentages for each column\n",
    "priority_percentages = [0.2] * X_encoded.shape[1]\n",
    "\n",
    "# Multiply features with priority weights\n",
    "weighted_X = X_encoded * priority_percentages\n",
    "\n",
    "# Step 2: Define and train the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(weighted_X, y_encoded)\n",
    "\n",
    "# Step 3: Save the model\n",
    "joblib.dump(model, 'random_forest_model.pkl')\n",
    "\n",
    "# Step 4: Load the model\n",
    "loaded_model = joblib.load('random_forest_model.pkl')\n",
    "\n",
    "# Step 5: Prompt user to input new data\n",
    "print(\"Please provide the following information:\")\n",
    "location = input(\"Location: \")\n",
    "gender = input(\"Gender: \")\n",
    "age = float(input(\"Age: \"))\n",
    "bmi = float(input(\"BMI: \"))\n",
    "allergy = input(\"Allergy: \")\n",
    "symptoms = input(\"Symptoms: \")\n",
    "duration = float(input(\"Duration of Symptoms: \"))\n",
    "disease_info = input(\"Current Disease Info: \")\n",
    "occupation = input(\"Occupation: \")\n",
    "smoker = input(\"Smoker (yes/no): \")\n",
    "bp_range = input(\"BP Range: \")\n",
    "temp_range = input(\"Temp Range: \")\n",
    "\n",
    "# Preprocess the new data\n",
    "input_data = pd.DataFrame({\n",
    "    'Location': [location],\n",
    "    'Gender': [gender],\n",
    "    'Age': [age],\n",
    "    'BMI': [bmi],\n",
    "    'Allergy': [allergy],\n",
    "    'Symptoms': [symptoms],\n",
    "    'Duration of Symptoms': [duration],\n",
    "    'Current Disease Info': [disease_info],\n",
    "    'Occupation': [occupation],\n",
    "    'Smoker': [smoker],\n",
    "    'BP Range': [bp_range],\n",
    "    'Temp Range': [temp_range]\n",
    "})\n",
    "\n",
    "# Handle missing values in new data\n",
    "input_data_filled = pd.DataFrame(imputer.transform(input_data), columns=input_data.columns)\n",
    "\n",
    "X_new = input_data_filled[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "X_new_encoded = encoder.transform(X_new)\n",
    "\n",
    "# Create priority weights for new data\n",
    "priority_weights_new = np.array(priority_percentages) / np.sum(priority_percentages)\n",
    "\n",
    "# Apply priority weights to new data\n",
    "weighted_X_new = X_new_encoded * priority_weights_new\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = loaded_model.predict(weighted_X_new)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9a119453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide the following information:\n",
      "Location: Ernakulam\n",
      "Gender: Male\n",
      "Age: 21\n",
      "BMI: 25\n",
      "Allergy: None\n",
      "Symptoms: mild fever,headache\n",
      "Duration of Symptoms: 4\n",
      "Current Disease Info: None\n",
      "Occupation: engineer\n",
      "Smoker (yes/no): no\n",
      "BP Range: 120/80\n",
      "Temp Range: 37.2°C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- Disease\n",
      "- Prescription\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 12 features, but SimpleImputer is expecting 14 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m input_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m: [location],\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m'\u001b[39m: [gender],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp Range\u001b[39m\u001b[38;5;124m'\u001b[39m: [temp_range]\n\u001b[0;32m     71\u001b[0m })\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Handle missing values in new data\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m input_data_filled \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(imputer\u001b[38;5;241m.\u001b[39mtransform(input_data), columns\u001b[38;5;241m=\u001b[39minput_data\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     76\u001b[0m X_new \u001b[38;5;241m=\u001b[39m input_data_filled[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBMI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllergy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymptoms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuration of Symptoms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCurrent Disease Info\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOccupation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSmoker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBP Range\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp Range\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     77\u001b[0m X_new_encoded \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(X_new)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\impute\\_base.py:496\u001b[0m, in \u001b[0;36mSimpleImputer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m\"\"\"Impute all missing values in `X`.\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \n\u001b[0;32m    483\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;124;03m    `X` with imputed values.\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    494\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 496\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_input(X, in_fit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    497\u001b[0m statistics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatistics_\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m statistics\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\impute\\_base.py:304\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[1;34m(self, X, in_fit)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m new_ve \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ve\n\u001b[0;32m    306\u001b[0m _check_inputs_dtype(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing_values)\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\impute\\_base.py:287\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[1;34m(self, X, in_fit)\u001b[0m\n\u001b[0;32m    284\u001b[0m     force_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 287\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    288\u001b[0m         X,\n\u001b[0;32m    289\u001b[0m         reset\u001b[38;5;241m=\u001b[39min_fit,\n\u001b[0;32m    290\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    291\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    292\u001b[0m         force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m    293\u001b[0m         copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy,\n\u001b[0;32m    294\u001b[0m     )\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not convert\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:600\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    597\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 12 features, but SimpleImputer is expecting 14 features as input."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Handling Missing Values (if any)\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "data_filled = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "\n",
    "X = data_filled[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y = data_filled[['Disease', 'Prescription']]\n",
    "\n",
    "# Encoding categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False, dtype=int)  # Set sparse=False and dtype=int\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# One-hot encoding the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = y.apply(label_encoder.fit_transform)\n",
    "\n",
    "# Define priority percentages for each column\n",
    "priority_percentages = [0.2] * X_encoded.shape[1]\n",
    "\n",
    "# Multiply features with priority weights\n",
    "weighted_X = X_encoded * priority_percentages\n",
    "\n",
    "# Step 2: Define and train the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(weighted_X, y_encoded)\n",
    "\n",
    "# Step 3: Save the model\n",
    "joblib.dump(model, 'random_forest_model.pkl')\n",
    "\n",
    "# Step 4: Load the model\n",
    "loaded_model = joblib.load('random_forest_model.pkl')\n",
    "\n",
    "# Step 5: Prompt user to input new data\n",
    "print(\"Please provide the following information:\")\n",
    "location = input(\"Location: \")\n",
    "gender = input(\"Gender: \")\n",
    "age = float(input(\"Age: \"))\n",
    "bmi = float(input(\"BMI: \"))\n",
    "allergy = input(\"Allergy: \")\n",
    "symptoms = input(\"Symptoms: \")\n",
    "duration = float(input(\"Duration of Symptoms: \"))\n",
    "disease_info = input(\"Current Disease Info: \")\n",
    "occupation = input(\"Occupation: \")\n",
    "smoker = input(\"Smoker (yes/no): \")\n",
    "bp_range = input(\"BP Range: \")\n",
    "temp_range = input(\"Temp Range: \")\n",
    "\n",
    "# Preprocess the new data\n",
    "input_data = pd.DataFrame({\n",
    "    'Location': [location],\n",
    "    'Gender': [gender],\n",
    "    'Age': [age],\n",
    "    'BMI': [bmi],\n",
    "    'Allergy': [allergy],\n",
    "    'Symptoms': [symptoms],\n",
    "    'Duration of Symptoms': [duration],\n",
    "    'Current Disease Info': [disease_info],\n",
    "    'Occupation': [occupation],\n",
    "    'Smoker': [smoker],\n",
    "    'BP Range': [bp_range],\n",
    "    'Temp Range': [temp_range]\n",
    "})\n",
    "\n",
    "# Handle missing values in new data\n",
    "input_data_filled = pd.DataFrame(imputer.transform(input_data), columns=input_data.columns)\n",
    "\n",
    "X_new = input_data_filled[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "X_new_encoded = encoder.transform(X_new)\n",
    "\n",
    "# Create priority weights for new data\n",
    "priority_weights_new = np.array(priority_percentages) / np.sum(priority_percentages)\n",
    "\n",
    "# Apply priority weights to new data\n",
    "weighted_X_new = X_new_encoded * priority_weights_new\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = loaded_model.predict(weighted_X_new)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "600d9bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 3]\n",
      " [0 3]\n",
      " [0 3]\n",
      " [0 3]\n",
      " [0 3]\n",
      " [0 3]\n",
      " [0 3]\n",
      " [0 3]\n",
      " [0 3]\n",
      " [0 3]\n",
      " [0 3]\n",
      " [0 3]\n",
      " [0 3]\n",
      " [0 3]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Handling Missing Values (if any)\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "data_filled = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "\n",
    "X = data_filled[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y = data_filled[['Disease', 'Prescription']]\n",
    "\n",
    "# Encoding categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False, dtype=int)  # Set sparse=False and dtype=int\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# One-hot encoding the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = y.apply(label_encoder.fit_transform)\n",
    "\n",
    "# Define priority percentages for each column\n",
    "priority_percentages = [0.2] * X_encoded.shape[1]\n",
    "\n",
    "# Multiply features with priority weights\n",
    "weighted_X = X_encoded * priority_percentages\n",
    "\n",
    "# Step 2: Define and train the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(weighted_X, y_encoded)\n",
    "\n",
    "# Step 3: Save the model\n",
    "joblib.dump(model, 'random_forest_model.pkl')\n",
    "\n",
    "# Step 4: Load the model\n",
    "loaded_model = joblib.load('random_forest_model.pkl')\n",
    "\n",
    "# Step 5: Load new data from file\n",
    "new_data = pd.read_csv('new_data.csv')\n",
    "\n",
    "# Handle missing values in new data\n",
    "new_data_filled = pd.DataFrame(imputer.transform(new_data), columns=new_data.columns)\n",
    "\n",
    "X_new = new_data_filled[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "X_new_encoded = encoder.transform(X_new)\n",
    "\n",
    "# Create priority weights for new data\n",
    "priority_weights_new = np.array(priority_percentages) / np.sum(priority_percentages)\n",
    "\n",
    "# Apply priority weights to new data\n",
    "weighted_X_new = X_new_encoded * priority_weights_new\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = loaded_model.predict(weighted_X_new)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c6a97ccf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sparse matrix length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m text_columns:\n\u001b[0;32m     26\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m---> 27\u001b[0m     X_train[column] \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(X_train[column])\n\u001b[0;32m     28\u001b[0m     X_test[column] \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test[column])\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Step 3: Feature Encoding\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3655\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3654\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item(key, value)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3832\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   3825\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3830\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   3831\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3832\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_column(value)\n\u001b[0;32m   3834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3835\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   3836\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   3837\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   3838\u001b[0m     ):\n\u001b[0;32m   3839\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   3840\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4529\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 4529\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   4530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\common.py:556\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequire_length_match\u001b[39m(data, index: Index):\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03m    Check the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m    557\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    558\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    559\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    560\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    561\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    562\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py:345\u001b[0m, in \u001b[0;36mspmatrix.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 345\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix length is ambiguous; use getnnz()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    346\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or shape[0]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: sparse matrix length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Handle missing values, if any\n",
    "data = data.dropna()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms',\n",
    "          'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y_disease = data['Disease']\n",
    "y_prescription = data['Prescription']\n",
    "X_train, X_test, y_train_disease, y_test_disease, y_train_prescription, y_test_prescription = train_test_split(\n",
    "    X, y_disease, y_prescription, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Text Data Processing\n",
    "text_columns = ['Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation']\n",
    "\n",
    "for column in text_columns:\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train[column] = vectorizer.fit_transform(X_train[column])\n",
    "    X_test[column] = vectorizer.transform(X_test[column])\n",
    "\n",
    "# Step 3: Feature Encoding\n",
    "categorical_columns = ['Location', 'Gender', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']\n",
    "\n",
    "for column in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_train[column] = le.fit_transform(X_train[column])\n",
    "    X_test[column] = le.transform(X_test[column])\n",
    "\n",
    "# Step 4: Model Training\n",
    "model_disease = RandomForestClassifier()\n",
    "model_prescription = RandomForestClassifier()\n",
    "\n",
    "model_disease.fit(X_train, y_train_disease)\n",
    "model_prescription.fit(X_train, y_train_prescription)\n",
    "\n",
    "# Step 5: Model Evaluation\n",
    "y_pred_disease = model_disease.predict(X_test)\n",
    "y_pred_prescription = model_prescription.predict(X_test)\n",
    "\n",
    "accuracy_disease = accuracy_score(y_test_disease, y_pred_disease)\n",
    "accuracy_prescription = accuracy_score(y_test_prescription, y_pred_prescription)\n",
    "\n",
    "print(f\"Accuracy - Disease: {accuracy_disease}\")\n",
    "print(f\"Accuracy - Prescription: {accuracy_prescription}\")\n",
    "\n",
    "# Step 6: Model Improvement\n",
    "# To improve the model, you can perform hyperparameter tuning, feature selection, or use more advanced NLP techniques like transformers.\n",
    "\n",
    "# Step 7: Iterate and Refine\n",
    "# Collect more data, update the model, and repeat the above steps to continuously improve the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3bbcacc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sparse matrix length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [58]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m text_columns:\n\u001b[0;32m     26\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m---> 27\u001b[0m     X_train[column] \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(X_train[column])\n\u001b[0;32m     28\u001b[0m     X_test[column] \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test[column])\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Convert sparse matrix to dense matrix\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3655\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3654\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item(key, value)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3832\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   3825\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3830\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   3831\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3832\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_column(value)\n\u001b[0;32m   3834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3835\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   3836\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   3837\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   3838\u001b[0m     ):\n\u001b[0;32m   3839\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   3840\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4529\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 4529\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   4530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\common.py:556\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequire_length_match\u001b[39m(data, index: Index):\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03m    Check the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m    557\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    558\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    559\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    560\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    561\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    562\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py:345\u001b[0m, in \u001b[0;36mspmatrix.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 345\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix length is ambiguous; use getnnz()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    346\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or shape[0]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: sparse matrix length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Handle missing values, if any\n",
    "data = data.dropna()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms',\n",
    "          'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y_disease = data['Disease']\n",
    "y_prescription = data['Prescription']\n",
    "X_train, X_test, y_train_disease, y_test_disease, y_train_prescription, y_test_prescription = train_test_split(\n",
    "    X, y_disease, y_prescription, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Text Data Processing\n",
    "text_columns = ['Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation']\n",
    "\n",
    "for column in text_columns:\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train[column] = vectorizer.fit_transform(X_train[column])\n",
    "    X_test[column] = vectorizer.transform(X_test[column])\n",
    "\n",
    "# Convert sparse matrix to dense matrix\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "\n",
    "# Step 3: Feature Encoding\n",
    "categorical_columns = ['Location', 'Gender', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']\n",
    "\n",
    "for column in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_train[column] = le.fit_transform(X_train[column])\n",
    "    X_test[column] = le.transform(X_test[column])\n",
    "\n",
    "# Step 4: Model Training\n",
    "model_disease = RandomForestClassifier()\n",
    "model_prescription = RandomForestClassifier()\n",
    "\n",
    "model_disease.fit(X_train, y_train_disease)\n",
    "model_prescription.fit(X_train, y_train_prescription)\n",
    "\n",
    "# Step 5: Model Evaluation\n",
    "y_pred_disease = model_disease.predict(X_test)\n",
    "y_pred_prescription = model_prescription.predict(X_test)\n",
    "\n",
    "accuracy_disease = accuracy_score(y_test_disease, y_pred_disease)\n",
    "accuracy_prescription = accuracy_score(y_test_prescription, y_pred_prescription)\n",
    "\n",
    "print(f\"Accuracy - Disease: {accuracy_disease}\")\n",
    "print(f\"Accuracy - Prescription: {accuracy_prescription}\")\n",
    "\n",
    "# Step 6: Model Improvement\n",
    "# To improve the model, you can perform hyperparameter tuning, feature selection, or use more advanced NLP techniques like transformers.\n",
    "\n",
    "# Step 7: Iterate and Refine\n",
    "# Collect more data, update the model, and repeat the above steps to continuously improve the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "382dfe86",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [59]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m text_columns:\n\u001b[0;32m     26\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m---> 27\u001b[0m     X_train[column] \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(X_train[column])\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     28\u001b[0m     X_test[column] \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test[column])\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Step 3: Feature Encoding\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2078\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2071\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2072\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2073\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2074\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2075\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2076\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2077\u001b[0m )\n\u001b[1;32m-> 2078\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[0;32m   2079\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2080\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2081\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1330\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1332\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1333\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1334\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1335\u001b[0m             )\n\u001b[0;32m   1336\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1341\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1208\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1210\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1211\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 69\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Handle missing values, if any\n",
    "data = data.dropna()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms',\n",
    "          'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y_disease = data['Disease']\n",
    "y_prescription = data['Prescription']\n",
    "X_train, X_test, y_train_disease, y_test_disease, y_train_prescription, y_test_prescription = train_test_split(\n",
    "    X, y_disease, y_prescription, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Text Data Processing\n",
    "text_columns = ['Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation']\n",
    "\n",
    "for column in text_columns:\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train[column] = vectorizer.fit_transform(X_train[column]).toarray()\n",
    "    X_test[column] = vectorizer.transform(X_test[column]).toarray()\n",
    "\n",
    "# Step 3: Feature Encoding\n",
    "categorical_columns = ['Location', 'Gender', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']\n",
    "\n",
    "for column in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_train[column] = le.fit_transform(X_train[column])\n",
    "    X_test[column] = le.transform(X_test[column])\n",
    "\n",
    "# Step 4: Model Training\n",
    "model_disease = RandomForestClassifier()\n",
    "model_prescription = RandomForestClassifier()\n",
    "\n",
    "model_disease.fit(X_train, y_train_disease)\n",
    "model_prescription.fit(X_train, y_train_prescription)\n",
    "\n",
    "# Step 5: Model Evaluation\n",
    "y_pred_disease = model_disease.predict(X_test)\n",
    "y_pred_prescription = model_prescription.predict(X_test)\n",
    "\n",
    "accuracy_disease = accuracy_score(y_test_disease, y_pred_disease)\n",
    "accuracy_prescription = accuracy_score(y_test_prescription, y_pred_prescription)\n",
    "\n",
    "print(f\"Accuracy - Disease: {accuracy_disease}\")\n",
    "print(f\"Accuracy - Prescription: {accuracy_prescription}\")\n",
    "\n",
    "# Step 6: Model Improvement\n",
    "# To improve the model, you can perform hyperparameter tuning, feature selection, or use more advanced NLP techniques like transformers.\n",
    "\n",
    "# Step 7: Iterate and Refine\n",
    "# Collect more data, update the model, and repeat the above steps to continuously improve the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1f0b1d2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: '120/75'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_encode.py:224\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_encode.py:164\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[1;34m(values, uniques)\u001b[0m\n\u001b[0;32m    163\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_encode.py:164\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    163\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_encode.py:158\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: '120/75'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [61]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m     le \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[0;32m     37\u001b[0m     X_train[column] \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mfit_transform(X_train[column])\n\u001b[1;32m---> 38\u001b[0m     X_test[column] \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mtransform(X_test[column])\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Step 4: Model Training\u001b[39;00m\n\u001b[0;32m     41\u001b[0m model_disease \u001b[38;5;241m=\u001b[39m RandomForestClassifier()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:138\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _encode(y, uniques\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_encode.py:226\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[1;31mValueError\u001b[0m: y contains previously unseen labels: '120/75'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Handle missing values, if any\n",
    "data = data.dropna()\n",
    "\n",
    "# Convert relevant columns to strings\n",
    "text_columns = ['Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation']\n",
    "data[text_columns] = data[text_columns].astype(str)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms',\n",
    "          'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y_disease = data['Disease']\n",
    "y_prescription = data['Prescription']\n",
    "X_train, X_test, y_train_disease, y_test_disease, y_train_prescription, y_test_prescription = train_test_split(\n",
    "    X, y_disease, y_prescription, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Text Data Processing\n",
    "for column in text_columns:\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train[column] = vectorizer.fit_transform(X_train[column]).toarray()\n",
    "    X_test[column] = vectorizer.transform(X_test[column]).toarray()\n",
    "\n",
    "# Step 3: Feature Encoding\n",
    "categorical_columns = ['Location', 'Gender', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']\n",
    "\n",
    "for column in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_train[column] = le.fit_transform(X_train[column])\n",
    "    X_test[column] = le.transform(X_test[column])\n",
    "\n",
    "# Step 4: Model Training\n",
    "model_disease = RandomForestClassifier()\n",
    "model_prescription = RandomForestClassifier()\n",
    "\n",
    "model_disease.fit(X_train, y_train_disease)\n",
    "model_prescription.fit(X_train, y_train_prescription)\n",
    "\n",
    "# Step 5: Model Evaluation\n",
    "y_pred_disease = model_disease.predict(X_test)\n",
    "y_pred_prescription = model_prescription.predict(X_test)\n",
    "\n",
    "accuracy_disease = accuracy_score(y_test_disease, y_pred_disease)\n",
    "accuracy_prescription = accuracy_score(y_test_prescription, y_pred_prescription)\n",
    "\n",
    "print(f\"Accuracy - Disease: {accuracy_disease}\")\n",
    "print(f\"Accuracy - Prescription: {accuracy_prescription}\")\n",
    "\n",
    "# Step 6: Model Improvement\n",
    "# To improve the model, you can perform hyperparameter tuning, feature selection, or use more advanced NLP techniques like transformers.\n",
    "\n",
    "# Step 7: Iterate and Refine\n",
    "# Collect more data, update the model, and repeat the above steps to continuously improve the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7e3025fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: '120/75'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_encode.py:224\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_encode.py:164\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[1;34m(values, uniques)\u001b[0m\n\u001b[0;32m    163\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_encode.py:164\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    163\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_encode.py:158\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: '120/75'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [62]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m     le \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[0;32m     43\u001b[0m     X_train[column] \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mfit_transform(X_train[column])\n\u001b[1;32m---> 44\u001b[0m     X_test[column] \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mtransform(X_test[column])\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Step 4: Model Training\u001b[39;00m\n\u001b[0;32m     47\u001b[0m model_disease \u001b[38;5;241m=\u001b[39m RandomForestClassifier()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:138\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _encode(y, uniques\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_encode.py:226\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[1;31mValueError\u001b[0m: y contains previously unseen labels: '120/75'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Handle missing values, if any\n",
    "data = data.dropna()\n",
    "\n",
    "# Convert relevant columns to strings\n",
    "text_columns = ['Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation']\n",
    "data[text_columns] = data[text_columns].astype(str)\n",
    "\n",
    "# Encode target variables\n",
    "le_disease = LabelEncoder()\n",
    "le_prescription = LabelEncoder()\n",
    "data['Disease'] = le_disease.fit_transform(data['Disease'])\n",
    "data['Prescription'] = le_prescription.fit_transform(data['Prescription'])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms',\n",
    "          'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y_disease = data['Disease']\n",
    "y_prescription = data['Prescription']\n",
    "X_train, X_test, y_train_disease, y_test_disease, y_train_prescription, y_test_prescription = train_test_split(\n",
    "    X, y_disease, y_prescription, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Text Data Processing\n",
    "for column in text_columns:\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train[column] = vectorizer.fit_transform(X_train[column]).toarray()\n",
    "    X_test[column] = vectorizer.transform(X_test[column]).toarray()\n",
    "\n",
    "# Step 3: Feature Encoding\n",
    "categorical_columns = ['Location', 'Gender', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']\n",
    "\n",
    "for column in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_train[column] = le.fit_transform(X_train[column])\n",
    "    X_test[column] = le.transform(X_test[column])\n",
    "\n",
    "# Step 4: Model Training\n",
    "model_disease = RandomForestClassifier()\n",
    "model_prescription = RandomForestClassifier()\n",
    "\n",
    "model_disease.fit(X_train, y_train_disease)\n",
    "model_prescription.fit(X_train, y_train_prescription)\n",
    "\n",
    "# Step 5: Model Evaluation\n",
    "y_pred_disease = model_disease.predict(X_test)\n",
    "y_pred_prescription = model_prescription.predict(X_test)\n",
    "\n",
    "accuracy_disease = accuracy_score(y_test_disease, y_pred_disease)\n",
    "accuracy_prescription = accuracy_score(y_test_prescription, y_pred_prescription)\n",
    "\n",
    "print(f\"Accuracy - Disease: {accuracy_disease}\")\n",
    "print(f\"Accuracy - Prescription: {accuracy_prescription}\")\n",
    "\n",
    "# Step 6: Model Improvement\n",
    "# To improve the model, you can perform hyperparameter tuning, feature selection, or use more advanced NLP techniques like transformers.\n",
    "\n",
    "# Step 7: Iterate and Refine\n",
    "# Collect more data, update the model, and repeat the above steps to continuously improve the\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "71208869",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:163\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(left, right)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:239\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate(op, op_str, a, b)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:69\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m     68\u001b[0m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op(a, b)\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Step 2: Text Data Processing\u001b[39;00m\n\u001b[0;32m     25\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m---> 26\u001b[0m X_train_tfidf \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllergy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymptoms\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuration of Symptoms\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCurrent Disease Info\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOccupation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     27\u001b[0m X_test_tfidf \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllergy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymptoms\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuration of Symptoms\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCurrent Disease Info\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOccupation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Step 3: Multi-label Encoding\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\ops\\common.py:70\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     68\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\arraylike.py:100\u001b[0m, in \u001b[0;36mOpsMixin.__add__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__add__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__add__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arith_method(other, operator\u001b[38;5;241m.\u001b[39madd)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:5639\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   5637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[0;32m   5638\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39malign_method_SERIES(\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m-> 5639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m base\u001b[38;5;241m.\u001b[39mIndexOpsMixin\u001b[38;5;241m.\u001b[39m_arith_method(\u001b[38;5;28mself\u001b[39m, other, op)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\base.py:1295\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   1292\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m ensure_wrapped_if_datetimelike(rvalues)\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1295\u001b[0m     result \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39marithmetic_op(lvalues, rvalues, op)\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(result, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:222\u001b[0m, in \u001b[0;36marithmetic_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# TODO we should handle EAs consistently and move this check before the if/else\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# (https://github.com/pandas-dev/pandas/issues/41165)\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     _bool_arith_check(op, left, right)\n\u001b[1;32m--> 222\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(left, right, op)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:170\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (is_object_dtype(left\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m is_object_dtype(right)):\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;66;03m# For object dtype, fallback to a masked operation (only operating\u001b[39;00m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;66;03m#  on the non-missing values)\u001b[39;00m\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m         result \u001b[38;5;241m=\u001b[39m _masked_arith_op(left, right, op)\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:108\u001b[0m, in \u001b[0;36m_masked_arith_op\u001b[1;34m(x, y, op)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# See GH#5284, GH#5035, GH#19448 for historical reference\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m--> 108\u001b[0m         result[mask] \u001b[38;5;241m=\u001b[39m op(xrav[mask], yrav[mask])\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(y):\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Handle missing values, if any\n",
    "data = data.dropna()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms',\n",
    "          'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y_disease = data['Disease'].apply(lambda x: [x])  # Convert single disease label to list\n",
    "y_prescription = data['Prescription'].apply(lambda x: [x])  # Convert single prescription label to list\n",
    "\n",
    "X_train, X_test, y_train_disease, y_test_disease, y_train_prescription, y_test_prescription = train_test_split(\n",
    "    X, y_disease, y_prescription, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Text Data Processing\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train['Allergy'] + ' ' + X_train['Symptoms'] + ' ' + X_train['Duration of Symptoms'] + ' ' + X_train['Current Disease Info'] + ' ' + X_train['Occupation'])\n",
    "X_test_tfidf = vectorizer.transform(X_test['Allergy'] + ' ' + X_test['Symptoms'] + ' ' + X_test['Duration of Symptoms'] + ' ' + X_test['Current Disease Info'] + ' ' + X_test['Occupation'])\n",
    "\n",
    "# Step 3: Multi-label Encoding\n",
    "mlb_disease = MultiLabelBinarizer()\n",
    "mlb_prescription = MultiLabelBinarizer()\n",
    "y_train_disease_encoded = mlb_disease.fit_transform(y_train_disease)\n",
    "y_train_prescription_encoded = mlb_prescription.fit_transform(y_train_prescription)\n",
    "\n",
    "# Step 4: Model Training\n",
    "classifier_disease = OneVsRestClassifier(RandomForestClassifier())\n",
    "classifier_prescription = OneVsRestClassifier(RandomForestClassifier())\n",
    "\n",
    "classifier_disease.fit(X_train_tfidf, y_train_disease_encoded)\n",
    "classifier_prescription.fit(X_train_tfidf, y_train_prescription_encoded)\n",
    "\n",
    "# Step 5: Model Evaluation\n",
    "y_pred_disease_encoded = classifier_disease.predict(X_test_tfidf)\n",
    "y_pred_prescription_encoded = classifier_prescription.predict(X_test_tfidf)\n",
    "\n",
    "y_pred_disease = mlb_disease.inverse_transform(y_pred_disease_encoded)\n",
    "y_pred_prescription = mlb_prescription.inverse_transform(y_pred_prescription_encoded)\n",
    "\n",
    "accuracy_disease = accuracy_score(y_test_disease, y_pred_disease)\n",
    "accuracy_prescription = accuracy_score(y_test_prescription, y_pred_prescription)\n",
    "\n",
    "print(f\"Accuracy - Disease: {accuracy_disease}\")\n",
    "print(f\"Accuracy - Prescription: {accuracy_prescription}\")\n",
    "\n",
    "# Step 6: Model Improvement\n",
    "# To improve the model, you can perform hyperparameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "276d02db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [68]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m y_pred_disease \u001b[38;5;241m=\u001b[39m mlb_disease\u001b[38;5;241m.\u001b[39minverse_transform(y_pred_disease_encoded)\n\u001b[0;32m     50\u001b[0m y_pred_prescription \u001b[38;5;241m=\u001b[39m mlb_prescription\u001b[38;5;241m.\u001b[39minverse_transform(y_pred_prescription_encoded)\n\u001b[1;32m---> 52\u001b[0m accuracy_disease \u001b[38;5;241m=\u001b[39m accuracy_score(y_test_disease, y_pred_disease)\n\u001b[0;32m     53\u001b[0m accuracy_prescription \u001b[38;5;241m=\u001b[39m accuracy_score(y_test_prescription, y_pred_prescription)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy - Disease: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_disease\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:211\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    212\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:85\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m---> 85\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\multiclass.py:307\u001b[0m, in \u001b[0;36mtype_of_target\u001b[1;34m(y, input_name)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y[\u001b[38;5;241m0\u001b[39m], Sequence)\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    306\u001b[0m     ):\n\u001b[1;32m--> 307\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    308\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou appear to be using a legacy multi-label data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    309\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m representation. Sequence of sequences are no\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    310\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m longer supported; use a binary array or sparse\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    311\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m matrix instead - the MultiLabelBinarizer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    312\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m transformer can convert to this format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    313\u001b[0m         )\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Handle missing values, if any\n",
    "data = data.dropna()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms',\n",
    "          'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y_disease = data['Disease'].apply(lambda x: [x])  # Convert single disease label to list\n",
    "y_prescription = data['Prescription'].apply(lambda x: [x])  # Convert single prescription label to list\n",
    "\n",
    "X_train, X_test, y_train_disease, y_test_disease, y_train_prescription, y_test_prescription = train_test_split(\n",
    "    X, y_disease, y_prescription, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Text Data Processing\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_text = X_train['Allergy'].astype(str) + ' ' + X_train['Symptoms'].astype(str) + ' ' + X_train['Duration of Symptoms'].astype(str) + ' ' + X_train['Current Disease Info'].astype(str) + ' ' + X_train['Occupation'].astype(str)\n",
    "X_test_text = X_test['Allergy'].astype(str) + ' ' + X_test['Symptoms'].astype(str) + ' ' + X_test['Duration of Symptoms'].astype(str) + ' ' + X_test['Current Disease Info'].astype(str) + ' ' + X_test['Occupation'].astype(str)\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_text)\n",
    "X_test_tfidf = vectorizer.transform(X_test_text)\n",
    "\n",
    "# Step 3: Multi-label Encoding\n",
    "mlb_disease = MultiLabelBinarizer()\n",
    "mlb_prescription = MultiLabelBinarizer()\n",
    "y_train_disease_encoded = mlb_disease.fit_transform(y_train_disease)\n",
    "y_train_prescription_encoded = mlb_prescription.fit_transform(y_train_prescription)\n",
    "\n",
    "# Step 4: Model Training\n",
    "classifier_disease = OneVsRestClassifier(RandomForestClassifier())\n",
    "classifier_prescription = OneVsRestClassifier(RandomForestClassifier())\n",
    "\n",
    "classifier_disease.fit(X_train_tfidf, y_train_disease_encoded)\n",
    "classifier_prescription.fit(X_train_tfidf, y_train_prescription_encoded)\n",
    "\n",
    "# Step 5: Model Evaluation\n",
    "y_pred_disease_encoded = classifier_disease.predict(X_test_tfidf)\n",
    "y_pred_prescription_encoded = classifier_prescription.predict(X_test_tfidf)\n",
    "\n",
    "y_pred_disease = mlb_disease.inverse_transform(y_pred_disease_encoded)\n",
    "y_pred_prescription = mlb_prescription.inverse_transform(y_pred_prescription_encoded)\n",
    "\n",
    "accuracy_disease = accuracy_score(y_test_disease, y_pred_disease)\n",
    "accuracy_prescription = accuracy_score(y_test_prescription, y_pred_prescription)\n",
    "\n",
    "print(f\"Accuracy - Disease: {accuracy_disease}\")\n",
    "print(f\"Accuracy - Prescription: {accuracy_prescription}\")\n",
    "\n",
    "# Step 6: Model Improvement\n",
    "# To improve the model, you can perform hyperparameter tuning, feature selection, or use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c76f401e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [69]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m y_pred_disease \u001b[38;5;241m=\u001b[39m mlb_disease\u001b[38;5;241m.\u001b[39minverse_transform(y_pred_disease_encoded)\n\u001b[0;32m     50\u001b[0m y_pred_prescription \u001b[38;5;241m=\u001b[39m mlb_prescription\u001b[38;5;241m.\u001b[39minverse_transform(y_pred_prescription_encoded)\n\u001b[1;32m---> 52\u001b[0m accuracy_disease \u001b[38;5;241m=\u001b[39m accuracy_score(y_test_disease\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [x]), y_pred_disease)\n\u001b[0;32m     53\u001b[0m accuracy_prescription \u001b[38;5;241m=\u001b[39m accuracy_score(y_test_prescription\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [x]), y_pred_prescription)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy - Disease: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_disease\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:211\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    212\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:85\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m---> 85\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\multiclass.py:307\u001b[0m, in \u001b[0;36mtype_of_target\u001b[1;34m(y, input_name)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y[\u001b[38;5;241m0\u001b[39m], Sequence)\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    306\u001b[0m     ):\n\u001b[1;32m--> 307\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    308\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou appear to be using a legacy multi-label data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    309\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m representation. Sequence of sequences are no\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    310\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m longer supported; use a binary array or sparse\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    311\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m matrix instead - the MultiLabelBinarizer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    312\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m transformer can convert to this format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    313\u001b[0m         )\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Handle missing values, if any\n",
    "data = data.dropna()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms',\n",
    "          'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y_disease = data['Disease']\n",
    "y_prescription = data['Prescription']\n",
    "\n",
    "X_train, X_test, y_train_disease, y_test_disease, y_train_prescription, y_test_prescription = train_test_split(\n",
    "    X, y_disease, y_prescription, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Text Data Processing\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_text = X_train['Allergy'].astype(str) + ' ' + X_train['Symptoms'].astype(str) + ' ' + X_train['Duration of Symptoms'].astype(str) + ' ' + X_train['Current Disease Info'].astype(str) + ' ' + X_train['Occupation'].astype(str)\n",
    "X_test_text = X_test['Allergy'].astype(str) + ' ' + X_test['Symptoms'].astype(str) + ' ' + X_test['Duration of Symptoms'].astype(str) + ' ' + X_test['Current Disease Info'].astype(str) + ' ' + X_test['Occupation'].astype(str)\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_text)\n",
    "X_test_tfidf = vectorizer.transform(X_test_text)\n",
    "\n",
    "# Step 3: Multi-label Encoding\n",
    "mlb_disease = MultiLabelBinarizer()\n",
    "mlb_prescription = MultiLabelBinarizer()\n",
    "y_train_disease_encoded = mlb_disease.fit_transform(y_train_disease.apply(lambda x: [x]))\n",
    "y_train_prescription_encoded = mlb_prescription.fit_transform(y_train_prescription.apply(lambda x: [x]))\n",
    "\n",
    "# Step 4: Model Training\n",
    "classifier_disease = OneVsRestClassifier(RandomForestClassifier())\n",
    "classifier_prescription = OneVsRestClassifier(RandomForestClassifier())\n",
    "\n",
    "classifier_disease.fit(X_train_tfidf, y_train_disease_encoded)\n",
    "classifier_prescription.fit(X_train_tfidf, y_train_prescription_encoded)\n",
    "\n",
    "# Step 5: Model Evaluation\n",
    "y_pred_disease_encoded = classifier_disease.predict(X_test_tfidf)\n",
    "y_pred_prescription_encoded = classifier_prescription.predict(X_test_tfidf)\n",
    "\n",
    "y_pred_disease = mlb_disease.inverse_transform(y_pred_disease_encoded)\n",
    "y_pred_prescription = mlb_prescription.inverse_transform(y_pred_prescription_encoded)\n",
    "\n",
    "accuracy_disease = accuracy_score(y_test_disease.apply(lambda x: [x]), y_pred_disease)\n",
    "accuracy_prescription = accuracy_score(y_test_prescription.apply(lambda x: [x]), y_pred_prescription)\n",
    "\n",
    "print(f\"Accuracy - Disease: {accuracy_disease}\")\n",
    "print(f\"Accuracy - Prescription: {accuracy_prescription}\")\n",
    "\n",
    "# Step 6: Model Improvement\n",
    "# To improve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b9c449cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (54,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [70]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m y_pred_disease \u001b[38;5;241m=\u001b[39m mlb_disease\u001b[38;5;241m.\u001b[39minverse_transform(y_pred_disease_encoded)\n\u001b[0;32m     50\u001b[0m y_pred_prescription \u001b[38;5;241m=\u001b[39m mlb_prescription\u001b[38;5;241m.\u001b[39minverse_transform(y_pred_prescription_encoded)\n\u001b[1;32m---> 52\u001b[0m accuracy_disease \u001b[38;5;241m=\u001b[39m accuracy_score(y_test_disease, y_pred_disease)\n\u001b[0;32m     53\u001b[0m accuracy_prescription \u001b[38;5;241m=\u001b[39m accuracy_score(y_test_prescription, y_pred_prescription)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy - Disease: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_disease\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:211\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    212\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     84\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m     85\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 86\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\multiclass.py:286\u001b[0m, in \u001b[0;36mtype_of_target\u001b[1;34m(y, input_name)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse_pandas:\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseSeries\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseArray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_multilabel(y):\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# DeprecationWarning will be replaced by ValueError, see NEP 34\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\multiclass.py:152\u001b[0m, in \u001b[0;36mis_multilabel\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    150\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mVisibleDeprecationWarning)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 152\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(y)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m np\u001b[38;5;241m.\u001b[39mVisibleDeprecationWarning:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;66;03m# dtype=object should be provided explicitly for ragged arrays,\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# see NEP 34\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (54,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Handle missing values, if any\n",
    "data = data.dropna()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms',\n",
    "          'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y_disease = data['Disease']\n",
    "y_prescription = data['Prescription']\n",
    "\n",
    "X_train, X_test, y_train_disease, y_test_disease, y_train_prescription, y_test_prescription = train_test_split(\n",
    "    X, y_disease, y_prescription, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Text Data Processing\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_text = X_train['Allergy'].astype(str) + ' ' + X_train['Symptoms'].astype(str) + ' ' + X_train['Duration of Symptoms'].astype(str) + ' ' + X_train['Current Disease Info'].astype(str) + ' ' + X_train['Occupation'].astype(str)\n",
    "X_test_text = X_test['Allergy'].astype(str) + ' ' + X_test['Symptoms'].astype(str) + ' ' + X_test['Duration of Symptoms'].astype(str) + ' ' + X_test['Current Disease Info'].astype(str) + ' ' + X_test['Occupation'].astype(str)\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_text)\n",
    "X_test_tfidf = vectorizer.transform(X_test_text)\n",
    "\n",
    "# Step 3: Multi-label Encoding\n",
    "mlb_disease = MultiLabelBinarizer()\n",
    "mlb_prescription = MultiLabelBinarizer()\n",
    "y_train_disease_encoded = mlb_disease.fit_transform(y_train_disease.apply(lambda x: [x]))\n",
    "y_train_prescription_encoded = mlb_prescription.fit_transform(y_train_prescription.apply(lambda x: [x]))\n",
    "\n",
    "# Step 4: Model Training\n",
    "classifier_disease = OneVsRestClassifier(RandomForestClassifier())\n",
    "classifier_prescription = OneVsRestClassifier(RandomForestClassifier())\n",
    "\n",
    "classifier_disease.fit(X_train_tfidf, y_train_disease_encoded)\n",
    "classifier_prescription.fit(X_train_tfidf, y_train_prescription_encoded)\n",
    "\n",
    "# Step 5: Model Evaluation\n",
    "y_pred_disease_encoded = classifier_disease.predict(X_test_tfidf)\n",
    "y_pred_prescription_encoded = classifier_prescription.predict(X_test_tfidf)\n",
    "\n",
    "y_pred_disease = mlb_disease.inverse_transform(y_pred_disease_encoded)\n",
    "y_pred_prescription = mlb_prescription.inverse_transform(y_pred_prescription_encoded)\n",
    "\n",
    "accuracy_disease = accuracy_score(y_test_disease, y_pred_disease)\n",
    "accuracy_prescription = accuracy_score(y_test_prescription, y_pred_prescription)\n",
    "\n",
    "print(f\"Accuracy - Disease: {accuracy_disease}\")\n",
    "print(f\"Accuracy - Prescription: {accuracy_prescription}\")\n",
    "\n",
    "# Step 6: Model Improvement\n",
    "# To improve the model, you can perform hyperparameter tuning, feature selection, or use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "efd063b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - Disease: 0.0\n",
      "Accuracy - Prescription: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Handle missing values, if any\n",
    "data = data.dropna()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms',\n",
    "          'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y_disease = data['Disease']\n",
    "y_prescription = data['Prescription']\n",
    "\n",
    "X_train, X_test, y_train_disease, y_test_disease, y_train_prescription, y_test_prescription = train_test_split(\n",
    "    X, y_disease, y_prescription, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Text Data Processing\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_text = X_train['Allergy'].astype(str) + ' ' + X_train['Symptoms'].astype(str) + ' ' + X_train['Duration of Symptoms'].astype(str) + ' ' + X_train['Current Disease Info'].astype(str) + ' ' + X_train['Occupation'].astype(str)\n",
    "X_test_text = X_test['Allergy'].astype(str) + ' ' + X_test['Symptoms'].astype(str) + ' ' + X_test['Duration of Symptoms'].astype(str) + ' ' + X_test['Current Disease Info'].astype(str) + ' ' + X_test['Occupation'].astype(str)\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_text)\n",
    "X_test_tfidf = vectorizer.transform(X_test_text)\n",
    "\n",
    "# Step 3: Multi-label Encoding\n",
    "mlb_disease = MultiLabelBinarizer()\n",
    "mlb_prescription = MultiLabelBinarizer()\n",
    "y_train_disease_encoded = mlb_disease.fit_transform(y_train_disease.apply(lambda x: [x]))\n",
    "y_train_prescription_encoded = mlb_prescription.fit_transform(y_train_prescription.apply(lambda x: [x]))\n",
    "\n",
    "# Step 4: Model Training\n",
    "classifier_disease = OneVsRestClassifier(RandomForestClassifier())\n",
    "classifier_prescription = OneVsRestClassifier(RandomForestClassifier())\n",
    "\n",
    "classifier_disease.fit(X_train_tfidf, y_train_disease_encoded)\n",
    "classifier_prescription.fit(X_train_tfidf, y_train_prescription_encoded)\n",
    "\n",
    "# Step 5: Model Evaluation\n",
    "y_pred_disease_encoded = classifier_disease.predict(X_test_tfidf)\n",
    "y_pred_prescription_encoded = classifier_prescription.predict(X_test_tfidf)\n",
    "\n",
    "# Convert binary arrays to sequences of labels\n",
    "y_pred_disease = mlb_disease.inverse_transform(y_pred_disease_encoded)\n",
    "y_pred_prescription = mlb_prescription.inverse_transform(y_pred_prescription_encoded)\n",
    "\n",
    "# Calculate accuracy manually\n",
    "accuracy_disease = sum(set(pred) == set(true) for pred, true in zip(y_pred_disease, y_test_disease)) / len(y_test_disease)\n",
    "accuracy_prescription = sum(set(pred) == set(true) for pred, true in zip(y_pred_prescription, y_test_prescription)) / len(y_test_prescription)\n",
    "\n",
    "print(f\"Accuracy - Disease: {accuracy_disease}\")\n",
    "print(f\"Accuracy - Prescription: {accuracy_prescription}\")\n",
    "\n",
    "# Step 6: Model Improvement\n",
    "# To improve the model, you can perform hyperparameter tuning, feature selection, or use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e31da841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\AppData\\Local\\Temp\\ipykernel_26708\\4011269892.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[column] = encoder.fit_transform(X[column])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 17ms/step - loss: 19.0134 - accuracy: 0.0583 - val_loss: 15.4599 - val_accuracy: 0.0370\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 12.3932 - accuracy: 0.0500 - val_loss: 10.4206 - val_accuracy: 0.0370\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 8.8270 - accuracy: 0.0417 - val_loss: 8.4306 - val_accuracy: 0.0370\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.5193 - accuracy: 0.0333 - val_loss: 7.3433 - val_accuracy: 0.0370\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 6.9735 - accuracy: 0.0500 - val_loss: 7.0985 - val_accuracy: 0.0370\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.0402 - accuracy: 0.0500 - val_loss: 7.3818 - val_accuracy: 0.0370\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.2074 - accuracy: 0.0792 - val_loss: 7.6006 - val_accuracy: 0.1111\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 7.5768 - accuracy: 0.1167 - val_loss: 8.0512 - val_accuracy: 0.1481\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 7.9918 - accuracy: 0.1458 - val_loss: 8.3992 - val_accuracy: 0.2222\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 8.6264 - accuracy: 0.1792 - val_loss: 8.9518 - val_accuracy: 0.2222\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 8.9518 - accuracy: 0.2222\n",
      "Test Loss: 8.95175552368164\n",
      "Test Accuracy: 0.2222222238779068\n",
      "Model saved successfully.\n",
      "Model loaded successfully.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 'Kozhikode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_encode.py:224\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_encode.py:164\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[1;34m(values, uniques)\u001b[0m\n\u001b[0;32m    163\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_encode.py:164\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    163\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_encode.py:158\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Kozhikode'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [74]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m new_data\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_data[column]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m---> 59\u001b[0m         new_data[column] \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mtransform(new_data[column])\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Make predictions using the loaded model\u001b[39;00m\n\u001b[0;32m     62\u001b[0m predictions \u001b[38;5;241m=\u001b[39m loaded_model\u001b[38;5;241m.\u001b[39mpredict(new_data)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:138\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _encode(y, uniques\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_encode.py:226\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[1;31mValueError\u001b[0m: y contains previously unseen labels: 'Kozhikode'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "data = pd.read_csv('data.csv')\n",
    "X = data[['Location', 'Gender', 'Age', 'BMI', 'Allergy', 'Symptoms', 'Duration of Symptoms', 'Current Disease Info', 'Occupation', 'Smoker', 'BP Range', 'Temp Range']]\n",
    "y = data[['Disease', 'Prescription']]\n",
    "\n",
    "# Encoding categorical features\n",
    "encoder = LabelEncoder()\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == object:\n",
    "        X[column] = encoder.fit_transform(X[column])\n",
    "\n",
    "# One-hot encoding the target variable\n",
    "y_encoded = pd.get_dummies(y)\n",
    "\n",
    "# Step 2: Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=X.shape[1]))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(y_encoded.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Step 3: Model Training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.1, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Step 4: Model Evaluation\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Step 5: Save the model\n",
    "model.save('model.h5')\n",
    "print(\"Model saved successfully.\")\n",
    "\n",
    "# Step 6: Load the saved model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "loaded_model = load_model('model.h5')\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Step 7: Perform predictions using the loaded model\n",
    "# Assuming you have new data for prediction stored in a DataFrame called 'new_data'\n",
    "# Preprocess the new data similar to how you preprocessed the training data\n",
    "\n",
    "# Encoding categorical features\n",
    "for column in new_data.columns:\n",
    "    if new_data[column].dtype == object:\n",
    "        new_data[column] = encoder.transform(new_data[column])\n",
    "\n",
    "# Make predictions using the loaded model\n",
    "predictions = loaded_model.predict(new_data)\n",
    "\n",
    "# Convert the predicted probabilities back to categorical labels\n",
    "predicted_labels = y_encoded.columns[np.argmax(predictions, axis=1)]\n",
    "print(\"Predicted labels:\", predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a2108f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
